{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e39d7293-7fb4-4ebc-b1b1-d8ce04ce415f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… XGB binary model trained.\n",
      "âœ… Logistic regression calibration trained.\n",
      "ðŸ”¹ Train: 17252 in-topic predictions\n",
      "ðŸ”¹ Test: 4171 in-topic predictions\n",
      "Multiclass: Unique labels: [  0. 543. 544. 546. 547. 550. 552. 554. 556. 600. 602.]\n",
      "Number of classes: 11\n",
      "Reduced multiclass training set shape: (3450, 384)\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best SVM parameters: {'C': 10, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "Best SVM macro F1 score: 0.5633535425810002\n",
      "Multiclass classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.25      0.33      1174\n",
      "           1       0.52      0.71      0.60       520\n",
      "           2       0.35      0.34      0.35       430\n",
      "           3       0.60      0.78      0.68       407\n",
      "           4       0.80      0.66      0.72       371\n",
      "           5       0.53      0.73      0.62       425\n",
      "           6       0.58      0.31      0.40        58\n",
      "           7       0.34      0.31      0.33        48\n",
      "           8       0.46      0.32      0.38        41\n",
      "           9       0.67      0.80      0.73       169\n",
      "          10       0.67      0.84      0.75       528\n",
      "\n",
      "    accuracy                           0.55      4171\n",
      "   macro avg       0.54      0.55      0.53      4171\n",
      "weighted avg       0.54      0.55      0.53      4171\n",
      "\n",
      "Multiclass confusion matrix:\n",
      "[[298 155 120 156  35 148  11  19  10  55 167]\n",
      " [ 56 367  57   8  11   4   0   1   0   0  16]\n",
      " [ 58 124 148  17   8  65   0   2   2   2   4]\n",
      " [ 48   8  12 316   1  16   0   0   0   1   5]\n",
      " [ 32  41  38   0 245   7   0   1   0   1   6]\n",
      " [ 45   2  36  17   3 311   2   0   3   4   2]\n",
      " [ 15   0   3   1   0  16  18   0   0   4   1]\n",
      " [ 16   2   0   2   0   2   0  15   0   0  11]\n",
      " [ 18   2   1   1   0   6   0   0  13   0   0]\n",
      " [ 11   1   5   2   3   4   0   2   0 136   5]\n",
      " [ 51   9   4   8   0   7   0   4   0   1 444]]\n",
      "Hierarchical predictions sample: ['out-of-topic' 'out-of-topic' 'out-of-topic' 'out-of-topic'\n",
      " 'out-of-topic' 'out-of-topic' 'out-of-topic' '0' 'out-of-topic' '0']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "#########################################\n",
    "# Load Data\n",
    "#########################################\n",
    "df_train = pd.read_csv('df_train_sbert.csv')\n",
    "df_test = pd.read_csv('df_test_sbert.csv')\n",
    "\n",
    "def parse_embedding(embedding):\n",
    "    if isinstance(embedding, str):  # If stored as a string\n",
    "        embedding = embedding.replace(\"[\", \"\").replace(\"]\", \"\")  # Remove brackets\n",
    "        return np.array([float(x) for x in embedding.split()])  # Convert to float array\n",
    "    return np.array(embedding)  # If already numeric, keep as is\n",
    "\n",
    "# Convert all embeddings properly\n",
    "df_train[\"sbert_embedding\"] = df_train[\"sbert_embedding\"].apply(parse_embedding)\n",
    "df_test[\"sbert_embedding\"] = df_test[\"sbert_embedding\"].apply(parse_embedding)\n",
    "\n",
    "#########################################\n",
    "# Step 1: Train Binary Classifier (XGB + Logistic)\n",
    "#########################################\n",
    "# Convert embeddings to NumPy arrays\n",
    "X_train_bin = np.vstack(df_train['sbert_embedding'].apply(lambda x: np.array(x, dtype=np.float32)))\n",
    "X_test_bin  = np.vstack(df_test['sbert_embedding'].apply(lambda x: np.array(x, dtype=np.float32)))\n",
    "y_train_bin = df_train['label'].values\n",
    "y_test_bin  = df_test['label'].values\n",
    "\n",
    "# Train XGBoost binary classifier\n",
    "xgb_bin = XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    random_state=42,\n",
    "    eval_metric=\"logloss\",\n",
    "    learning_rate=0.1,\n",
    "    max_depth=7,\n",
    "    n_estimators=200,\n",
    "    n_jobs=-1\n",
    ")\n",
    "xgb_bin.fit(X_train_bin, y_train_bin)\n",
    "print(\"âœ… XGB binary model trained.\")\n",
    "\n",
    "# Train logistic regression for probability calibration\n",
    "y_prob_train_bin = xgb_bin.predict_proba(X_train_bin)[:, 1].reshape(-1, 1)\n",
    "y_prob_test_bin  = xgb_bin.predict_proba(X_test_bin)[:, 1].reshape(-1, 1)\n",
    "\n",
    "log_reg_bin = make_pipeline(StandardScaler(), LogisticRegression(class_weight=\"balanced\", max_iter=1000))\n",
    "log_reg_bin.fit(y_prob_train_bin, y_train_bin)\n",
    "print(\"âœ… Logistic regression calibration trained.\")\n",
    "\n",
    "def binary_ensemble_predict(X):\n",
    "    \"\"\"Return binary predictions (0: out-of-topic, 1: in-topic) using the ensemble.\"\"\"\n",
    "    probs = xgb_bin.predict_proba(X)[:, 1].reshape(-1, 1)\n",
    "    return log_reg_bin.predict(probs)\n",
    "\n",
    "#########################################\n",
    "# Step 2: Prepare Multiclass Dataset (Only True & False Positives)\n",
    "#########################################\n",
    "binary_preds_train = binary_ensemble_predict(X_train_bin)\n",
    "binary_preds_test  = binary_ensemble_predict(X_test_bin)\n",
    "\n",
    "# Get indices of in-topic predictions (true or false positives)\n",
    "train_in_topic_indices = np.where(binary_preds_train == 1)[0]\n",
    "test_in_topic_indices  = np.where(binary_preds_test == 1)[0]\n",
    "\n",
    "print(f\"ðŸ”¹ Train: {len(train_in_topic_indices)} in-topic predictions\")\n",
    "print(f\"ðŸ”¹ Test: {len(test_in_topic_indices)} in-topic predictions\")\n",
    "\n",
    "# Subset df_train to only include these predictions\n",
    "df_train_multi = df_train.iloc[train_in_topic_indices].copy()\n",
    "df_test_multi  = df_test.iloc[test_in_topic_indices].copy()\n",
    "\n",
    "# Set false positives to '0' in topic_id\n",
    "df_train_multi['topic_id'] = df_train_multi.apply(\n",
    "    lambda row: row['topic_id'] if row['label'] == 1 else 0.0, axis=1\n",
    ")\n",
    "df_test_multi['topic_id'] = df_test_multi.apply(\n",
    "    lambda row: row['topic_id'] if row['label'] == 1 else 0.0, axis=1\n",
    ")\n",
    "\n",
    "# Convert embeddings\n",
    "X_train_multi = np.vstack(df_train_multi['sbert_embedding'].apply(lambda x: np.array(x, dtype=np.float32)))\n",
    "X_test_multi  = np.vstack(df_test_multi['sbert_embedding'].apply(lambda x: np.array(x, dtype=np.float32)))\n",
    "\n",
    "# Map topic_id to integers\n",
    "unique_labels = np.sort(np.unique(df_train_multi['topic_id']))\n",
    "topic_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "y_train_multi = np.array([topic_mapping[label] for label in df_train_multi['topic_id']])\n",
    "y_test_multi  = np.array([topic_mapping[label] for label in df_test_multi['topic_id']])\n",
    "num_classes = len(unique_labels)\n",
    "\n",
    "print(\"Multiclass: Unique labels:\", unique_labels)\n",
    "print(\"Number of classes:\", num_classes)\n",
    "\n",
    "#########################################\n",
    "# Step 3: Train Multiclass Classifier (SVM)\n",
    "#########################################\n",
    "subset_fraction = 0.2\n",
    "X_train_multi_subset, _, y_train_multi_subset, _ = train_test_split(\n",
    "    X_train_multi, y_train_multi, test_size=1 - subset_fraction, random_state=42, stratify=y_train_multi\n",
    ")\n",
    "print(\"Reduced multiclass training set shape:\", X_train_multi_subset.shape)\n",
    "\n",
    "param_grid = [\n",
    "    {'kernel': ['rbf'], 'C': [1, 10], 'gamma': [0.001, 0.01]}\n",
    "]\n",
    "\n",
    "svm = SVC(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=svm,\n",
    "    param_grid=param_grid,\n",
    "    scoring='f1_macro',\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_search.fit(X_train_multi_subset, y_train_multi_subset)\n",
    "print(\"Best SVM parameters:\", grid_search.best_params_)\n",
    "print(\"Best SVM macro F1 score:\", grid_search.best_score_)\n",
    "\n",
    "best_svm = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate on full test set\n",
    "y_pred_multi = best_svm.predict(X_test_multi)\n",
    "print(\"Multiclass classification report:\")\n",
    "print(classification_report(y_test_multi, y_pred_multi))\n",
    "print(\"Multiclass confusion matrix:\")\n",
    "print(confusion_matrix(y_test_multi, y_pred_multi))\n",
    "\n",
    "#########################################\n",
    "# Step 4: Hierarchical Prediction\n",
    "#########################################\n",
    "def hierarchical_predict(X):\n",
    "    \"\"\"\n",
    "    Hierarchical prediction:\n",
    "      1. Use the binary ensemble to decide if an instance is in-topic (1) or out-of-topic (0).\n",
    "      2. For in-topic instances, use the best SVM to predict a topic (multiclass).\n",
    "      3. For out-of-topic instances, assign \"out-of-topic\".\n",
    "    \n",
    "    Args:\n",
    "      X: Input embeddings (should be the same for binary and multiclass tasks).\n",
    "    \n",
    "    Returns:\n",
    "      final_preds: Array of final predictions (either an integer topic label or \"out-of-topic\").\n",
    "    \"\"\"\n",
    "    binary_preds = binary_ensemble_predict(X)\n",
    "    final_preds = []\n",
    "    for i, bp in enumerate(binary_preds):\n",
    "        if bp == 1:\n",
    "            topic_pred = best_svm.predict(X[i].reshape(1, -1))[0]\n",
    "            final_preds.append(topic_pred)\n",
    "        else:\n",
    "            final_preds.append(\"out-of-topic\")\n",
    "    return np.array(final_preds)\n",
    "\n",
    "final_predictions = hierarchical_predict(X_test_bin)\n",
    "print(\"Hierarchical predictions sample:\", final_predictions[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13659855-396c-4a93-b4b4-c15476ae56be",
   "metadata": {},
   "source": [
    "Can observe that in the binary stage, there are too many false negative (i.e. in-topic predicted as out-of-topic), which results in the underperform on the downstream multiclassifcation. So in the following step, I turn to use logistic regression on the first classifer, which has the highest recall on label True (i.e. there should be more false positive than false negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1fbf3ab3-6cbc-4e27-8971-05334e5de6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Binary classifier (Logistic Regression) trained.\n",
      "ðŸ”¹ Train: 23818 in-topic predictions\n",
      "ðŸ”¹ Test: 5997 in-topic predictions\n",
      "Multiclass: Unique labels: [  0. 543. 544. 546. 547. 550. 552. 554. 556. 600. 602.]\n",
      "Number of classes: 11\n",
      "Reduced multiclass training set shape: (4763, 384)\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "âœ… Best SVM parameters: {'C': 10, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "âœ… Best SVM macro F1 score: 0.43514134383724556\n",
      "Multiclass classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.81      0.68      2530\n",
      "           1       0.58      0.57      0.58       559\n",
      "           2       0.46      0.18      0.25       513\n",
      "           3       0.67      0.48      0.56       492\n",
      "           4       0.86      0.57      0.69       413\n",
      "           5       0.62      0.52      0.57       501\n",
      "           6       0.55      0.09      0.15        67\n",
      "           7       1.00      0.03      0.06        70\n",
      "           8       0.86      0.11      0.20        53\n",
      "           9       0.72      0.60      0.66       207\n",
      "          10       0.70      0.64      0.67       592\n",
      "\n",
      "    accuracy                           0.62      5997\n",
      "   macro avg       0.69      0.42      0.46      5997\n",
      "weighted avg       0.63      0.62      0.60      5997\n",
      "\n",
      "Multiclass confusion matrix:\n",
      "[[2048   86   33   99   26   68    1    0    1   43  125]\n",
      " [ 177  321   36    2    6    3    0    0    0    0   14]\n",
      " [ 255   98   90    5    3   57    1    0    0    0    4]\n",
      " [ 236    3    2  238    0    9    0    0    0    0    4]\n",
      " [ 119   29   14    1  235    8    0    0    0    2    5]\n",
      " [ 206    2   13    9    2  263    3    0    0    2    1]\n",
      " [  41    0    1    0    0   16    6    0    0    2    1]\n",
      " [  63    0    0    0    0    0    0    2    0    0    5]\n",
      " [  44    1    0    0    0    1    0    0    6    0    1]\n",
      " [  73    2    1    0    0    1    0    0    0  125    5]\n",
      " [ 191   13    4    2    0    1    0    0    0    0  381]]\n",
      "Hierarchical predictions sample: ['out-of-topic' 'out-of-topic' 'out-of-topic' 'out-of-topic'\n",
      " 'out-of-topic' 'out-of-topic' 'out-of-topic' '0' 'out-of-topic' '0']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "############################################\n",
    "# Load Data\n",
    "############################################\n",
    "df_train = pd.read_csv('df_train_sbert.csv')\n",
    "df_test = pd.read_csv('df_test_sbert.csv')\n",
    "\n",
    "def parse_embedding(embedding):\n",
    "    if isinstance(embedding, str):  # If stored as a string\n",
    "        embedding = embedding.replace(\"[\", \"\").replace(\"]\", \"\")  # Remove brackets\n",
    "        return np.array([float(x) for x in embedding.split()])  # Convert to float array\n",
    "    return np.array(embedding)  # If already numeric, keep as is\n",
    "\n",
    "# Convert all embeddings properly\n",
    "df_train[\"sbert_embedding\"] = df_train[\"sbert_embedding\"].apply(parse_embedding)\n",
    "df_test[\"sbert_embedding\"] = df_test[\"sbert_embedding\"].apply(parse_embedding)\n",
    "\n",
    "############################################\n",
    "# Step 1: Train Binary Classifier (Logistic Regression)\n",
    "############################################\n",
    "X_train_bin = np.vstack(df_train[\"sbert_embedding\"].apply(lambda x: np.array(x, dtype=np.float32)))\n",
    "X_test_bin  = np.vstack(df_test[\"sbert_embedding\"].apply(lambda x: np.array(x, dtype=np.float32)))\n",
    "y_train_bin = df_train[\"label\"].values\n",
    "y_test_bin  = df_test[\"label\"].values\n",
    "\n",
    "# Train Logistic Regression for binary classification\n",
    "binary_model = LogisticRegression(C=1, max_iter=1000, class_weight=\"balanced\")\n",
    "binary_model.fit(X_train_bin, y_train_bin)\n",
    "print(\"âœ… Binary classifier (Logistic Regression) trained.\")\n",
    "\n",
    "def binary_predict(X):\n",
    "    \"\"\"Predict binary labels (0: out-of-topic, 1: in-topic).\"\"\"\n",
    "    return binary_model.predict(X)\n",
    "\n",
    "############################################\n",
    "# Step 2: Prepare Multiclass Dataset (Only True & False Positives)\n",
    "############################################\n",
    "\n",
    "binary_preds_train = binary_predict(X_train_bin)\n",
    "binary_preds_test  = binary_predict(X_test_bin)\n",
    "\n",
    "# Get indices of in-topic predictions (true or false positives)\n",
    "train_in_topic_indices = np.where(binary_preds_train == 1)[0]\n",
    "test_in_topic_indices  = np.where(binary_preds_test == 1)[0]\n",
    "\n",
    "print(f\"ðŸ”¹ Train: {len(train_in_topic_indices)} in-topic predictions\")\n",
    "print(f\"ðŸ”¹ Test: {len(test_in_topic_indices)} in-topic predictions\")\n",
    "\n",
    "# Subset df_train to only include these predictions\n",
    "df_train_multi = df_train.iloc[train_in_topic_indices].copy()\n",
    "df_test_multi  = df_test.iloc[test_in_topic_indices].copy()\n",
    "\n",
    "# Set false positives to '0' in topic_id\n",
    "df_train_multi['topic_id'] = df_train_multi.apply(\n",
    "    lambda row: row['topic_id'] if row['label'] == 1 else 0.0, axis=1\n",
    ")\n",
    "df_test_multi['topic_id'] = df_test_multi.apply(\n",
    "    lambda row: row['topic_id'] if row['label'] == 1 else 0.0, axis=1\n",
    ")\n",
    "\n",
    "# Convert embeddings\n",
    "X_train_multi = np.vstack(df_train_multi['sbert_embedding'].apply(lambda x: np.array(x, dtype=np.float32)))\n",
    "X_test_multi  = np.vstack(df_test_multi['sbert_embedding'].apply(lambda x: np.array(x, dtype=np.float32)))\n",
    "\n",
    "# Map topic_id to integers\n",
    "unique_labels = np.sort(np.unique(df_train_multi['topic_id']))\n",
    "topic_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "y_train_multi = np.array([topic_mapping[label] for label in df_train_multi['topic_id']])\n",
    "y_test_multi  = np.array([topic_mapping[label] for label in df_test_multi['topic_id']])\n",
    "num_classes = len(unique_labels)\n",
    "\n",
    "print(\"Multiclass: Unique labels:\", unique_labels)\n",
    "print(\"Number of classes:\", num_classes)\n",
    "\n",
    "############################################\n",
    "# Step 3: Train Multiclass Classifier (SVM)\n",
    "############################################\n",
    "subset_fraction = 0.2\n",
    "X_train_multi_subset, _, y_train_multi_subset, _ = train_test_split(\n",
    "    X_train_multi, y_train_multi, test_size=1 - subset_fraction, random_state=42, stratify=y_train_multi\n",
    ")\n",
    "print(\"Reduced multiclass training set shape:\", X_train_multi_subset.shape)\n",
    "\n",
    "param_grid = [\n",
    "    {'kernel': ['rbf'], 'C': [1, 10], 'gamma': [0.001, 0.01]}\n",
    "]\n",
    "\n",
    "svm = SVC(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=svm,\n",
    "    param_grid=param_grid,\n",
    "    scoring='f1_macro',\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_search.fit(X_train_multi_subset, y_train_multi_subset)\n",
    "print(\"âœ… Best SVM parameters:\", grid_search.best_params_)\n",
    "print(\"âœ… Best SVM macro F1 score:\", grid_search.best_score_)\n",
    "\n",
    "best_svm = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate on full test set\n",
    "y_pred_multi = best_svm.predict(X_test_multi)\n",
    "print(\"Multiclass classification report:\")\n",
    "print(classification_report(y_test_multi, y_pred_multi))\n",
    "print(\"Multiclass confusion matrix:\")\n",
    "print(confusion_matrix(y_test_multi, y_pred_multi))\n",
    "\n",
    "#########################################\n",
    "# Step 4: Hierarchical Prediction\n",
    "#########################################\n",
    "def hierarchical_predict(X):\n",
    "    \"\"\"\n",
    "    Hierarchical prediction:\n",
    "      1. Use the binary ensemble to decide if an instance is in-topic (1) or out-of-topic (0).\n",
    "      2. For in-topic instances, use the best SVM to predict a topic (multiclass).\n",
    "      3. For out-of-topic instances, assign \"out-of-topic\".\n",
    "    \n",
    "    Args:\n",
    "      X: Input embeddings (should be the same for binary and multiclass tasks).\n",
    "    \n",
    "    Returns:\n",
    "      final_preds: Array of final predictions (either an integer topic label or \"out-of-topic\").\n",
    "    \"\"\"\n",
    "    binary_preds = binary_ensemble_predict(X)\n",
    "    final_preds = []\n",
    "    for i, bp in enumerate(binary_preds):\n",
    "        if bp == 1:\n",
    "            topic_pred = best_svm.predict(X[i].reshape(1, -1))[0]\n",
    "            final_preds.append(topic_pred)\n",
    "        else:\n",
    "            final_preds.append(\"out-of-topic\")\n",
    "    return np.array(final_preds)\n",
    "\n",
    "final_predictions = hierarchical_predict(X_test_bin)\n",
    "print(\"Hierarchical predictions sample:\", final_predictions[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ef236e-5709-47af-b93f-5e253ac4b202",
   "metadata": {},
   "source": [
    "When you tune the binary classifier to maximize recall, it tends to pass many samples as \"in-topic\" even if they're actually out-of-topic. This increases the number of false positives that the multiclass classifier has to process. As a result, the multiclass stage ends up receiving a mix of genuine in-topic samples and many false positives, which can confuse it and lead to poorer performance overall (lower precision and lower F1 scores for the in-topic classes)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85debd4-445d-46b1-aae2-25e1467bd695",
   "metadata": {},
   "source": [
    "In this case, the downstream task (multiclassification) is noisy. Maybe boosting method is a more robust choice than SVM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aba53728-89cb-4582-8d80-ca733deeadb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Binary classifier (Logistic Regression) trained.\n",
      "ðŸ”¹ Train: 23818 in-topic predictions\n",
      "ðŸ”¹ Test: 5997 in-topic predictions\n",
      "Multiclass: Unique labels: [  0. 543. 544. 546. 547. 550. 552. 554. 556. 600. 602.]\n",
      "Number of classes: 11\n",
      "âœ… Multiclass XGBoost model trained.\n",
      "Multiclass classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.84      0.69      2530\n",
      "           1       0.58      0.54      0.56       559\n",
      "           2       0.38      0.19      0.25       513\n",
      "           3       0.67      0.48      0.56       492\n",
      "           4       0.85      0.53      0.65       413\n",
      "           5       0.61      0.48      0.54       501\n",
      "           6       0.73      0.12      0.21        67\n",
      "           7       0.42      0.07      0.12        70\n",
      "           8       0.50      0.15      0.23        53\n",
      "           9       0.80      0.55      0.65       207\n",
      "          10       0.72      0.57      0.64       592\n",
      "\n",
      "    accuracy                           0.61      5997\n",
      "   macro avg       0.62      0.41      0.46      5997\n",
      "weighted avg       0.62      0.61      0.59      5997\n",
      "\n",
      "Multiclass confusion matrix:\n",
      "[[2120   69   54   85   24   64    1    4    4   22   83]\n",
      " [ 190  301   40    6    4    2    0    0    0    1   15]\n",
      " [ 251   96   97    9    6   46    1    1    0    1    5]\n",
      " [ 223    5    8  236    0   13    0    0    3    0    4]\n",
      " [ 122   27   30    2  218    6    0    0    0    1    7]\n",
      " [ 220    4   21   10    1  240    1    0    0    2    2]\n",
      " [  40    0    3    0    0   15    8    0    0    1    0]\n",
      " [  55    0    0    0    0    1    0    5    0    0    9]\n",
      " [  40    1    0    0    0    3    0    0    8    0    1]\n",
      " [  86    0    1    0    1    2    0    0    0  113    4]\n",
      " [ 227   14    3    4    2    1    0    2    1    0  338]]\n",
      "Hierarchical predictions sample: ['out-of-topic' 'out-of-topic' 'out-of-topic' 'out-of-topic'\n",
      " 'out-of-topic' 'out-of-topic' 'out-of-topic' '0' 'out-of-topic' '0']\n",
      "[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time=   2.2s\n",
      "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   1.8s\n",
      "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   1.9s\n",
      "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   2.0s\n",
      "[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time=   2.0s\n",
      "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   2.0s\n",
      "[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time=   2.0s\n",
      "[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time=   2.1s\n",
      "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   2.1s\n",
      "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   2.1s\n",
      "[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time=   2.2s\n",
      "[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time=   2.3s\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "############################################\n",
    "# Load Data\n",
    "############################################\n",
    "df_train = pd.read_csv('df_train_sbert.csv')\n",
    "df_test = pd.read_csv('df_test_sbert.csv')\n",
    "\n",
    "def parse_embedding(embedding):\n",
    "    if isinstance(embedding, str):  # If stored as a string\n",
    "        embedding = embedding.replace(\"[\", \"\").replace(\"]\", \"\")  # Remove brackets\n",
    "        return np.array([float(x) for x in embedding.split()])  # Convert to float array\n",
    "    return np.array(embedding)  # If already numeric, keep as is\n",
    "\n",
    "# Convert all embeddings properly\n",
    "df_train[\"sbert_embedding\"] = df_train[\"sbert_embedding\"].apply(parse_embedding)\n",
    "df_test[\"sbert_embedding\"] = df_test[\"sbert_embedding\"].apply(parse_embedding)\n",
    "\n",
    "############################################\n",
    "# Stage 1: Train Binary Classifier (Logistic Regression)\n",
    "############################################\n",
    "X_train_bin = np.vstack(df_train[\"sbert_embedding\"].apply(lambda x: np.array(x, dtype=np.float32)))\n",
    "X_test_bin  = np.vstack(df_test[\"sbert_embedding\"].apply(lambda x: np.array(x, dtype=np.float32)))\n",
    "y_train_bin = df_train[\"label\"].values\n",
    "y_test_bin  = df_test[\"label\"].values\n",
    "\n",
    "# Train Logistic Regression for binary classification\n",
    "binary_model = LogisticRegression(C=1, max_iter=1000, class_weight=\"balanced\")\n",
    "binary_model.fit(X_train_bin, y_train_bin)\n",
    "print(\"âœ… Binary classifier (Logistic Regression) trained.\")\n",
    "\n",
    "def binary_predict(X):\n",
    "    \"\"\"Predict binary labels (0: out-of-topic, 1: in-topic).\"\"\"\n",
    "    return binary_model.predict(X)\n",
    "\n",
    "############################################\n",
    "# Stage 2: Prepare Multiclass Dataset (Only True & False Positives)\n",
    "############################################\n",
    "binary_preds_train = binary_predict(X_train_bin)\n",
    "binary_preds_test  = binary_predict(X_test_bin)\n",
    "\n",
    "# Get indices of in-topic predictions (true or false positives)\n",
    "train_in_topic_indices = np.where(binary_preds_train == 1)[0]\n",
    "test_in_topic_indices  = np.where(binary_preds_test == 1)[0]\n",
    "\n",
    "print(f\"ðŸ”¹ Train: {len(train_in_topic_indices)} in-topic predictions\")\n",
    "print(f\"ðŸ”¹ Test: {len(test_in_topic_indices)} in-topic predictions\")\n",
    "\n",
    "# Subset df_train to only include these predictions\n",
    "df_train_multi = df_train.iloc[train_in_topic_indices].copy()\n",
    "df_test_multi  = df_test.iloc[test_in_topic_indices].copy()\n",
    "\n",
    "# Set false positives to \"0\" in topic_id\n",
    "df_train_multi['topic_id'] = df_train_multi.apply(\n",
    "    lambda row: row['topic_id'] if row['label'] == 1 else 0.0, axis=1\n",
    ")\n",
    "df_test_multi['topic_id'] = df_test_multi.apply(\n",
    "    lambda row: row['topic_id'] if row['label'] == 1 else 0.0, axis=1\n",
    ")\n",
    "\n",
    "# Convert embeddings\n",
    "X_train_multi = np.vstack(df_train_multi[\"sbert_embedding\"].apply(lambda x: np.array(x, dtype=np.float32)))\n",
    "X_test_multi  = np.vstack(df_test_multi[\"sbert_embedding\"].apply(lambda x: np.array(x, dtype=np.float32)))\n",
    "\n",
    "# Map topic_id to contiguous integers\n",
    "unique_labels = np.sort(np.unique(df_train_multi[\"topic_id\"]))\n",
    "topic_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "y_train_multi = np.array([topic_mapping[label] for label in df_train_multi[\"topic_id\"]])\n",
    "y_test_multi  = np.array([topic_mapping[label] for label in df_test_multi[\"topic_id\"]])\n",
    "num_classes = len(unique_labels)\n",
    "\n",
    "print(\"Multiclass: Unique labels:\", unique_labels)\n",
    "print(\"Number of classes:\", num_classes)\n",
    "\n",
    "############################################\n",
    "# Stage 3: Train Multiclass Classifier (XGBoost)\n",
    "############################################\n",
    "# Use best params from previous tuning\n",
    "multi_model = XGBClassifier(\n",
    "    objective='multi:softmax',\n",
    "    num_class=num_classes,\n",
    "    eval_metric='mlogloss',\n",
    "    learning_rate=0.1,\n",
    "    max_depth=7,\n",
    "    n_estimators=200,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "multi_model.fit(X_train_multi, y_train_multi)\n",
    "print(\"âœ… Multiclass XGBoost model trained.\")\n",
    "\n",
    "# Evaluate on full test set\n",
    "y_pred_multi = multi_model.predict(X_test_multi)\n",
    "print(\"Multiclass classification report:\")\n",
    "print(classification_report(y_test_multi, y_pred_multi))\n",
    "print(\"Multiclass confusion matrix:\")\n",
    "print(confusion_matrix(y_test_multi, y_pred_multi))\n",
    "\n",
    "#########################################\n",
    "# Step 4: Hierarchical Prediction\n",
    "#########################################\n",
    "def hierarchical_predict(X):\n",
    "    \"\"\"\n",
    "    Hierarchical prediction:\n",
    "      1. Use the binary ensemble to decide if an instance is in-topic (1) or out-of-topic (0).\n",
    "      2. For in-topic instances, use the best SVM to predict a topic (multiclass).\n",
    "      3. For out-of-topic instances, assign \"out-of-topic\".\n",
    "    \n",
    "    Args:\n",
    "      X: Input embeddings (should be the same for binary and multiclass tasks).\n",
    "    \n",
    "    Returns:\n",
    "      final_preds: Array of final predictions (either an integer topic label or \"out-of-topic\").\n",
    "    \"\"\"\n",
    "    binary_preds = binary_ensemble_predict(X)\n",
    "    final_preds = []\n",
    "    for i, bp in enumerate(binary_preds):\n",
    "        if bp == 1:\n",
    "            topic_pred = best_svm.predict(X[i].reshape(1, -1))[0]\n",
    "            final_preds.append(topic_pred)\n",
    "        else:\n",
    "            final_preds.append(\"out-of-topic\")\n",
    "    return np.array(final_preds)\n",
    "\n",
    "final_predictions = hierarchical_predict(X_test_bin)\n",
    "print(\"Hierarchical predictions sample:\", final_predictions[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1690ff55-67e2-412c-bfc6-320d7eda1650",
   "metadata": {},
   "source": [
    "In your setup, the SVM for the multiclass stage appears to perform better than XGBoost. The SVM yields higher macro F1 scores and overall better classification performance on the in-topic classes, making it a preferable choice in the hierarchical pipeline.\n",
    "\n",
    "If the binary classifier is already tuned for high recall (even if it passes on more noise), using SVM for the multiclass stage seems to be a better option to mitigate that noise compared to XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424a4ce8-bf7a-4afd-9654-30342e3612c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
