{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "994f4781-db89-41ab-a37b-ebff55e2ccf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_train = pd.read_csv('df_train_sbert.csv')\n",
    "df_test = pd.read_csv('df_test_sbert.csv')\n",
    "df_train_label = df_train[df_train['label'] == True]\n",
    "df_test_label = df_test[df_test['label'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df5f13b6-0b5c-423d-95de-838644fe20a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4291/221399675.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train_label[\"sbert_embedding\"] = df_train_label[\"sbert_embedding\"].apply(parse_embedding)\n",
      "/tmp/ipykernel_4291/221399675.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test_label[\"sbert_embedding\"] = df_test_label[\"sbert_embedding\"].apply(parse_embedding)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to safely convert string embeddings to numpy arrays\n",
    "def parse_embedding(embedding):\n",
    "    if isinstance(embedding, str):  # If stored as a string\n",
    "        embedding = embedding.replace(\"[\", \"\").replace(\"]\", \"\")  # Remove brackets\n",
    "        return np.array([float(x) for x in embedding.split()])  # Convert to float array\n",
    "    return np.array(embedding)  # If already numeric, keep as is\n",
    "\n",
    "# Convert all embeddings properly\n",
    "df_train_label[\"sbert_embedding\"] = df_train_label[\"sbert_embedding\"].apply(parse_embedding)\n",
    "df_test_label[\"sbert_embedding\"] = df_test_label[\"sbert_embedding\"].apply(parse_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f986c1dc-16e0-4d1d-aa33-f4ff6ba832e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train class distribution:\n",
      "topic_id\n",
      "547.0    0.103959\n",
      "550.0    0.103724\n",
      "544.0    0.102848\n",
      "546.0    0.102591\n",
      "554.0    0.102356\n",
      "552.0    0.102356\n",
      "543.0    0.101780\n",
      "602.0    0.101694\n",
      "556.0    0.100776\n",
      "600.0    0.077916\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Test class distribution:\n",
      "topic_id\n",
      "602.0    0.106128\n",
      "543.0    0.105786\n",
      "552.0    0.103475\n",
      "554.0    0.103475\n",
      "546.0    0.102533\n",
      "544.0    0.101506\n",
      "550.0    0.097997\n",
      "556.0    0.097398\n",
      "547.0    0.097056\n",
      "600.0    0.084646\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Concatenate the existing training and test sets into one DataFrame.\n",
    "df_all_label = pd.concat([df_train, df_test], ignore_index=True)\n",
    "\n",
    "# Now, re-split the combined dataset using stratification to maintain class distribution.\n",
    "# Replace 'topic_id' with 'label' if it's a binary task.\n",
    "df_train_label, df_test_label = train_test_split(df_all_label, test_size=0.2, random_state=42, stratify=df_all_label['label'])\n",
    "\n",
    "# Optionally, reset the index for convenience.\n",
    "df_train_label = df_train_label.reset_index(drop=True)\n",
    "df_test_label = df_test_label.reset_index(drop=True)\n",
    "\n",
    "# Check the new class distributions to ensure stratification worked as intended.\n",
    "print(\"Train class distribution:\")\n",
    "print(df_train_label['topic_id'].value_counts(normalize=True))\n",
    "print(\"\\nTest class distribution:\")\n",
    "print(df_test_label['topic_id'].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5f22c2e-25c1-4018-bf11-f085a04f8975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train class distribution:\n",
      "topic_id\n",
      "602.0    0.175889\n",
      "543.0    0.154646\n",
      "546.0    0.150320\n",
      "544.0    0.148318\n",
      "550.0    0.145864\n",
      "547.0    0.100342\n",
      "600.0    0.058824\n",
      "554.0    0.028863\n",
      "556.0    0.019177\n",
      "552.0    0.017757\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Test class distribution:\n",
      "topic_id\n",
      "602.0    0.175961\n",
      "550.0    0.152788\n",
      "543.0    0.149223\n",
      "546.0    0.145913\n",
      "544.0    0.141839\n",
      "547.0    0.110772\n",
      "600.0    0.058569\n",
      "554.0    0.029794\n",
      "552.0    0.018844\n",
      "556.0    0.016297\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df_train_label = df_train[df_train['label'] == True]\n",
    "df_test_label = df_test[df_test['label'] == True]\n",
    "\n",
    "print(\"Train class distribution:\")\n",
    "print(df_train_label['topic_id'].value_counts(normalize=True))\n",
    "print(\"\\nTest class distribution:\")\n",
    "print(df_test_label['topic_id'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8804c3-505d-45d4-b940-0af860524aaf",
   "metadata": {},
   "source": [
    "above some sanity check. Some topic just have few true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbb62353-9a8e-445a-a535-e2a0419bf521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert SBERT embeddings to 2D NumPy arrays\n",
    "X_train = np.vstack(df_train_label['sbert_embedding'].values)\n",
    "X_test  = np.vstack(df_test_label['sbert_embedding'].values)\n",
    "\n",
    "# Use 'topic_id' as labels for multiclass classification\n",
    "y_train = df_train_label['topic_id'].values\n",
    "y_test  = df_test_label['topic_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7280c06-1178-4adb-8845-1d34c522a7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .............................................C=0.01; total time=  12.0s\n",
      "[CV] END .............................................C=0.01; total time=  14.2s\n",
      "[CV] END .............................................C=0.01; total time=  11.6s\n",
      "[CV] END ..............................................C=0.1; total time=  19.6s\n",
      "[CV] END ..............................................C=0.1; total time=  24.0s\n",
      "[CV] END ..............................................C=0.1; total time=  22.5s\n",
      "[CV] END ................................................C=1; total time=  39.7s\n",
      "[CV] END ................................................C=1; total time=  45.2s\n",
      "[CV] END ................................................C=1; total time=  43.4s\n",
      "[CV] END ...............................................C=10; total time= 1.1min\n",
      "[CV] END ...............................................C=10; total time= 1.2min\n",
      "[CV] END ...............................................C=10; total time= 1.3min\n",
      "[CV] END ..............................................C=100; total time= 2.3min\n",
      "[CV] END ..............................................C=100; total time= 2.2min\n",
      "[CV] END ..............................................C=100; total time= 2.4min\n",
      "Best C: 1\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       543.0       0.68      0.73      0.70       586\n",
      "       544.0       0.51      0.39      0.45       557\n",
      "       546.0       0.84      0.81      0.82       573\n",
      "       547.0       0.81      0.76      0.78       435\n",
      "       550.0       0.74      0.71      0.73       600\n",
      "       552.0       0.39      0.77      0.52        74\n",
      "       554.0       0.45      0.79      0.57       117\n",
      "       556.0       0.49      0.86      0.62        64\n",
      "       600.0       0.85      0.91      0.88       230\n",
      "       602.0       0.89      0.80      0.84       691\n",
      "\n",
      "    accuracy                           0.72      3927\n",
      "   macro avg       0.67      0.75      0.69      3927\n",
      "weighted avg       0.73      0.72      0.72      3927\n",
      "\n",
      "Confusion Matrix:\n",
      "[[427  78  12  23   9   6   8   2   2  19]\n",
      " [132 220  28  27  69  24  20  16   7  14]\n",
      " [  9  27 462   7  24   6  10  18   0  10]\n",
      " [ 25  27   7 331  17   7   7   0   8   6]\n",
      " [ 12  50  25  10 425  34  18  13   7   6]\n",
      " [  0   4   1   0   5  57   3   0   2   2]\n",
      " [  2   1   1   1   4   2  93   1   1  11]\n",
      " [  2   4   0   0   2   1   0  55   0   0]\n",
      " [  0   1   2   4   3   4   4   0 210   2]\n",
      " [ 21  16  13   8  13   4  44   7  10 555]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Set up logistic regression for multiclass classification\n",
    "lr = LogisticRegression(multi_class='multinomial',\n",
    "                        solver='lbfgs',\n",
    "                        max_iter=1000,\n",
    "                        class_weight='balanced')\n",
    "\n",
    "# Define a grid for the regularization parameter C\n",
    "param_grid = {'C': [0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "# Use GridSearchCV to find the best C based on macro F1 score\n",
    "grid_search = GridSearchCV(lr, param_grid, scoring='f1_macro', cv=3, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best logistic regression model\n",
    "best_lr = grid_search.best_estimator_\n",
    "print(f\"Best C: {grid_search.best_params_['C']}\")\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = best_lr.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c26244a-8021-4933-8460-5cf5e34ec258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train dtype: float32 shape: (15487, 384)\n",
      "X_test dtype: float32 shape: (3927, 384)\n",
      "Unique labels: [543. 544. 546. 547. 550. 552. 554. 556. 600. 602.]\n",
      "Number of classes: 10\n",
      "Fitting 3 folds for each of 81 candidates, totalling 243 fits\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Convert each embedding to a NumPy array of type float32 before stacking\n",
    "X_train = np.vstack(df_train_label['sbert_embedding'].apply(lambda x: np.array(x, dtype=np.float32)))\n",
    "X_test  = np.vstack(df_test_label['sbert_embedding'].apply(lambda x: np.array(x, dtype=np.float32)))\n",
    "\n",
    "# Optionally, check the dtype and shape\n",
    "print(\"X_train dtype:\", X_train.dtype, \"shape:\", X_train.shape)\n",
    "print(\"X_test dtype:\", X_test.dtype, \"shape:\", X_test.shape)\n",
    "\n",
    "\n",
    "# Get original labels\n",
    "y_train_orig = df_train_label['topic_id'].values\n",
    "y_test_orig  = df_test_label['topic_id'].values\n",
    "\n",
    "# Create mapping from original labels to new labels (0, 1, 2, ...)\n",
    "unique_labels = np.sort(np.unique(y_train_orig))\n",
    "mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "\n",
    "# Map the labels\n",
    "y_train = np.array([mapping[label] for label in y_train_orig])\n",
    "y_test  = np.array([mapping[label] for label in y_test_orig])\n",
    "\n",
    "# Determine the number of classes (should equal len(unique_labels))\n",
    "num_classes = len(unique_labels)\n",
    "print(\"Unique labels:\", unique_labels)\n",
    "print(\"Number of classes:\", num_classes)\n",
    "\n",
    "# Define the parameter grid for XGBoost\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'gamma': [0, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "# Initialize the XGBoost classifier for multiclass classification.\n",
    "xgb_clf = XGBClassifier(\n",
    "    objective='multi:softmax',\n",
    "    num_class=num_classes,\n",
    "    eval_metric='mlogloss',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Set up GridSearchCV with macro F1 score as the metric.\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_clf,\n",
    "    param_grid=param_grid,\n",
    "    scoring='f1_macro',\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Perform the grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Display the best parameters and score\n",
    "print(\"Best parameters found:\", grid_search.best_params_)\n",
    "print(\"Best macro F1 score:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a13f1208-ef00-468e-b0be-099f9426ce32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_full dtype: float32 shape: (15487, 384)\n",
      "X_test dtype: float32 shape: (3927, 384)\n",
      "Unique labels: [543. 544. 546. 547. 550. 552. 554. 556. 600. 602.]\n",
      "Number of classes: 10\n",
      "Reduced X_train shape: (7743, 384)\n",
      "Reduced y_train shape: (7743,)\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Best parameters found: {'n_estimators': 200, 'max_depth': 5, 'learning_rate': 0.1, 'gamma': 0.1}\n",
      "Best macro F1 score: 0.618881277103864\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.74      0.66       586\n",
      "           1       0.39      0.36      0.38       557\n",
      "           2       0.79      0.80      0.80       573\n",
      "           3       0.86      0.62      0.72       435\n",
      "           4       0.65      0.77      0.70       600\n",
      "           5       0.84      0.28      0.42        74\n",
      "           6       0.68      0.35      0.46       117\n",
      "           7       0.75      0.47      0.58        64\n",
      "           8       0.88      0.72      0.79       230\n",
      "           9       0.77      0.86      0.82       691\n",
      "\n",
      "    accuracy                           0.68      3927\n",
      "   macro avg       0.72      0.60      0.63      3927\n",
      "weighted avg       0.69      0.68      0.68      3927\n",
      "\n",
      "Confusion Matrix:\n",
      "[[431  91  10  12  13   1   1   1   1  25]\n",
      " [164 202  35  17 101   2   4   2   5  25]\n",
      " [ 15  37 461   1  31   0   1   5   0  22]\n",
      " [ 38  73   7 271  27   0   1   0   2  16]\n",
      " [ 14  53  32   6 463   1   2   1   7  21]\n",
      " [  0  10   4   0  26  21   1   0   4   8]\n",
      " [ 11   9   5   1  16   0  41   0   3  31]\n",
      " [  2  11   6   1  10   0   0  30   0   4]\n",
      " [  8  16   4   1  12   0   1   0 165  23]\n",
      " [ 28  17  16   6  18   0   8   1   0 597]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Convert each embedding to a NumPy array of type float32 before stacking\n",
    "X_train_full = np.vstack(df_train_label['sbert_embedding'].apply(lambda x: np.array(x, dtype=np.float32)))\n",
    "X_test  = np.vstack(df_test_label['sbert_embedding'].apply(lambda x: np.array(x, dtype=np.float32)))\n",
    "\n",
    "# Check dtype and shape\n",
    "print(\"X_train_full dtype:\", X_train_full.dtype, \"shape:\", X_train_full.shape)\n",
    "print(\"X_test dtype:\", X_test.dtype, \"shape:\", X_test.shape)\n",
    "\n",
    "# Get original labels\n",
    "y_train_orig = df_train_label['topic_id'].values\n",
    "y_test_orig  = df_test_label['topic_id'].values\n",
    "\n",
    "# Create mapping from original labels to new contiguous labels (0, 1, 2, ...)\n",
    "unique_labels = np.sort(np.unique(y_train_orig))\n",
    "mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "y_train_full = np.array([mapping[label] for label in y_train_orig])\n",
    "y_test  = np.array([mapping[label] for label in y_test_orig])\n",
    "\n",
    "# Determine number of classes\n",
    "num_classes = len(unique_labels)\n",
    "print(\"Unique labels:\", unique_labels)\n",
    "print(\"Number of classes:\", num_classes)\n",
    "\n",
    "# **Reduce training size by sampling a subset (e.g., 20% of the full training data)**\n",
    "subset_fraction = 0.5  # Use 20% of training data\n",
    "X_train, _, y_train, _ = train_test_split(\n",
    "    X_train_full, y_train_full, \n",
    "    test_size=1 - subset_fraction, \n",
    "    random_state=42,\n",
    "    stratify=y_train_full\n",
    ")\n",
    "\n",
    "print(\"Reduced X_train shape:\", X_train.shape)\n",
    "print(\"Reduced y_train shape:\", y_train.shape)\n",
    "\n",
    "# Define a smaller parameter grid for RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'max_depth': [3, 5, 10],              \n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'n_estimators': [100, 200],\n",
    "    'gamma': [0, 0.1]\n",
    "}\n",
    "\n",
    "# Initialize the XGBoost classifier for multiclass classification.\n",
    "xgb_clf = XGBClassifier(\n",
    "    objective='multi:softmax',\n",
    "    num_class=num_classes,\n",
    "    eval_metric='mlogloss',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Set up RandomizedSearchCV with fewer iterations and 2-fold CV.\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_clf,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,              # try 10 random combinations\n",
    "    scoring='f1_macro',\n",
    "    cv=3,                   # use 3-fold CV to speed up\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Perform the randomized search on the reduced training set\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Display best parameters and score\n",
    "print(\"Best parameters found:\", random_search.best_params_)\n",
    "print(\"Best macro F1 score:\", random_search.best_score_)\n",
    "\n",
    "# Evaluate the best model on the full test set\n",
    "best_model = random_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "50a00cd3-c42b-40d9-85c1-bceae40b78ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full training set shape: (15487, 384)\n",
      "Unique labels: [543. 544. 546. 547. 550. 552. 554. 556. 600. 602.]\n",
      "Reduced training set shape: (7743, 384)\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "Best parameters found: {'C': 10, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "Best macro F1 score: 0.684086936436397\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.80      0.70       586\n",
      "           1       0.49      0.39      0.44       557\n",
      "           2       0.82      0.82      0.82       573\n",
      "           3       0.88      0.66      0.76       435\n",
      "           4       0.67      0.80      0.73       600\n",
      "           5       0.71      0.47      0.57        74\n",
      "           6       0.68      0.50      0.58       117\n",
      "           7       0.78      0.67      0.72        64\n",
      "           8       0.89      0.81      0.85       230\n",
      "           9       0.83      0.87      0.85       691\n",
      "\n",
      "    accuracy                           0.72      3927\n",
      "   macro avg       0.74      0.68      0.70      3927\n",
      "weighted avg       0.73      0.72      0.72      3927\n",
      "\n",
      "Confusion Matrix:\n",
      "[[467  57  10   9  16   1   3   0   2  21]\n",
      " [159 217  35  11  99   4   6   4   4  18]\n",
      " [ 16  32 468   3  33   0   1   4   1  15]\n",
      " [ 45  57   7 287  23   1   1   0   2  12]\n",
      " [ 23  42  25   6 479   4   0   2   7  12]\n",
      " [  1   6   3   0  23  35   2   0   2   2]\n",
      " [  6   3   3   2  12   1  59   1   2  28]\n",
      " [  3   5   2   0   7   0   0  43   0   4]\n",
      " [  6   8   3   2  10   3   1   0 186  11]\n",
      " [ 27  12  17   5  14   0  14   1   2 599]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Assume df_train_label has:\n",
    "# - 'sbert_embedding': SBERT embeddings (each a list/array)\n",
    "# - 'topic_id': multiclass labels (e.g., 543.0, 544.0, etc.)\n",
    "\n",
    "# Convert embeddings to a 2D NumPy array of type float32\n",
    "X_full = np.vstack(df_train_label['sbert_embedding'].apply(lambda x: np.array(x, dtype=np.float32)))\n",
    "y_orig = df_train_label['topic_id'].values\n",
    "\n",
    "# Remap original labels to contiguous integers (0, 1, 2, ...)\n",
    "unique_labels = np.sort(np.unique(y_orig))\n",
    "mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "y_full = np.array([mapping[label] for label in y_orig])\n",
    "\n",
    "print(\"Full training set shape:\", X_full.shape)\n",
    "print(\"Unique labels:\", unique_labels)\n",
    "\n",
    "# Reduce training size by sampling a subset (e.g., 20% of the data) using stratification.\n",
    "subset_fraction = 0.5\n",
    "X_subset, _, y_subset, _ = train_test_split(\n",
    "    X_full, y_full,\n",
    "    test_size=1 - subset_fraction,\n",
    "    random_state=42,\n",
    "    stratify=y_full\n",
    ")\n",
    "print(\"Reduced training set shape:\", X_subset.shape)\n",
    "\n",
    "# Define a parameter grid that explores both linear and RBF kernels.\n",
    "param_grid = [\n",
    "    {'kernel': ['linear'], 'C': [0.1, 1, 10]},\n",
    "    {'kernel': ['rbf'], 'C': [0.1, 1, 10], 'gamma': [0.001, 0.01, 0.1]}\n",
    "]\n",
    "\n",
    "# Initialize the SVM classifier.\n",
    "svm = SVC(random_state=42)\n",
    "\n",
    "# Set up GridSearchCV using 5-fold cross-validation and macro F1 as the scoring metric.\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=svm,\n",
    "    param_grid=param_grid,\n",
    "    scoring='f1_macro',\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Perform the grid search on the subset.\n",
    "grid_search.fit(X_subset, y_subset)\n",
    "\n",
    "# Display the best parameters and best macro F1 score.\n",
    "print(\"Best parameters found:\", grid_search.best_params_)\n",
    "print(\"Best macro F1 score:\", grid_search.best_score_)\n",
    "\n",
    "# Optionally, evaluate on the full test set if available:\n",
    "# Assuming you have a similar test set, you can process it as follows:\n",
    "# (Here, we assume df_test_label exists similarly with 'sbert_embedding' and 'topic_id'.)\n",
    "X_test = np.vstack(df_test_label['sbert_embedding'].apply(lambda x: np.array(x, dtype=np.float32)))\n",
    "y_test_orig = df_test_label['topic_id'].values\n",
    "y_test = np.array([mapping[label] for label in y_test_orig])\n",
    "\n",
    "y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c826cac-3d5c-4322-8477-13db05644458",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
