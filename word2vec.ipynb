{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "541b3d58-597c-45df-bb38-ff53db1e58bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import data_preprocessing  # Make sure it's already imported\n",
    "importlib.reload(data_preprocessing)  # Force reload\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import data_preprocessing\n",
    "from data_preprocessing import clean_text, correct_spelling, replace_emoji, lemmatize_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cff18b4-3bd9-45e2-b9ea-29a748a7569a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"dataset.parquet\")\n",
    "label_df = df[df['label'] == True]\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train_label, df_test_label = train_test_split(label_df, test_size=0.2,random_state=42)\n",
    "\n",
    "# data preprocessing, skip lemmanization for bert and sbert\n",
    "def process_text_pipeline(text, country):\n",
    "    text = clean_text(text)\n",
    "    text = correct_spelling(text, country)\n",
    "    text = replace_emoji(text, country)\n",
    "    text = lemmatize_text(text, country)\n",
    "    return text\n",
    "\n",
    "# process for df_train\n",
    "processed_texts = []\n",
    "# Iterate through the DataFrame rows\n",
    "for index, row in df_train_label.iterrows():\n",
    "    processed_text = process_text_pipeline(row['quote_text'], row['country_name'])\n",
    "    processed_texts.append(processed_text)\n",
    "\n",
    "# Assign lists back to the DataFrame\n",
    "df_train_label['processed_text_nsc'] = processed_texts\n",
    "\n",
    "# Repeat for df_test\n",
    "processed_texts = []\n",
    "\n",
    "for index, row in df_test_label.iterrows():\n",
    "    processed_text = process_text_pipeline(row['quote_text'], row['country_name'])\n",
    "    processed_texts.append(processed_text)\n",
    "\n",
    "df_test_label['processed_text_nsc'] = processed_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b4a23b-4904-4d44-9546-e6f0176f1d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Download and load the pretrained model\n",
    "model = api.load(\"word2vec-google-news-300\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70ea7bf1-43a6-4cf8-819f-ddb142aed447",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load language models\n",
    "nlp_en = spacy.load('en_core_web_sm')\n",
    "nlp_fr = spacy.load('fr_core_news_sm')\n",
    "\n",
    "# Tokenization function\n",
    "def spacy_tokenize(text, lang='en'):\n",
    "    nlp = nlp_fr if lang == 'fr' else nlp_en\n",
    "    return [token.text for token in nlp(text)]\n",
    "\n",
    "# Apply tokenization based on country_name\n",
    "df_train_label['tokenized_text'] = df_train_label.apply(\n",
    "    lambda row: spacy_tokenize(row['processed_text_nsc'], 'fr' if row['country_name'] == 'France' else 'en'),\n",
    "    axis=1\n",
    ")\n",
    "df_test_label['tokenized_text'] = df_test_label.apply(\n",
    "    lambda row: spacy_tokenize(row['processed_text_nsc'], 'fr' if row['country_name'] == 'France' else 'en'),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e382973-3d15-4bba-aec8-f703ab432f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word2vec_embedding(tokens, model):\n",
    "    \"\"\"\n",
    "    Compute the average Word2Vec embedding for a list of tokens.\n",
    "    \n",
    "    Parameters:\n",
    "        tokens (list): List of tokenized words from a sentence.\n",
    "        model (gensim KeyedVectors): Pretrained Word2Vec model.\n",
    "    \n",
    "    Returns:\n",
    "        numpy array: Averaged word vector of fixed size.\n",
    "    \"\"\"\n",
    "    valid_vectors = [model[token] for token in tokens if token in model]  # ✅ FIXED\n",
    "\n",
    "    if valid_vectors:\n",
    "        return np.mean(valid_vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)  # Ensure fixed-size output\n",
    "\n",
    "# Apply function to DataFrame\n",
    "df_train_label[\"word2vec_embedding\"] = df_train_label[\"tokenized_text\"].apply(lambda tokens: get_word2vec_embedding(tokens, model))\n",
    "df_test_label[\"word2vec_embedding\"] = df_test_label[\"tokenized_text\"].apply(lambda tokens: get_word2vec_embedding(tokens, model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "694b4319-9b3a-4e55-b1fb-66aac837cbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "df_train_label['data_word2vec_embedding'] = df_train_label['data_word2vec_embedding'].apply(lambda x: np.fromstring(x.strip(\"[]\"), sep=\" \") if isinstance(x, str) else np.array(x))\n",
    "# Stack embeddings properly\n",
    "X = np.vstack(df_train_label['data_word2vec_embedding'].values)\n",
    "\n",
    "# Apply K-Means clustering\n",
    "k = 11  # Number of clusters, 10 topics add out of topic\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "df_train_label['cluster'] = kmeans.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8a8f1d69-0136-4325-9446-360bcc75e806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              quote_text  cluster  topic_id  \\\n",
      "38030  The face cream was easy to pump out and I didn...        4     550.0   \n",
      "50679  Très bonne idée la recharge, pour partir en vo...       10     556.0   \n",
      "35584  J’utilise ce produit depuis 2 mois maintenant ...        1     546.0   \n",
      "44679  This set of 3 masks comes in a fully recycled ...        5     602.0   \n",
      "33645  Points forts    Format automatique pratique, s...       10     546.0   \n",
      "\n",
      "       matched_topic_id  \n",
      "38030             550.0  \n",
      "50679             602.0  \n",
      "35584             602.0  \n",
      "44679             602.0  \n",
      "33645             602.0  \n"
     ]
    }
   ],
   "source": [
    "# Group by cluster and topic_id to see distribution\n",
    "cluster_topic_distribution = df_train_label.groupby(['cluster', 'topic_id']).size().reset_index(name='count')\n",
    "\n",
    "# Find the most common topic_id in each cluster\n",
    "cluster_to_topic = cluster_topic_distribution.sort_values('count', ascending=False).drop_duplicates('cluster')\n",
    "\n",
    "# Create a mapping from cluster to topic_id\n",
    "cluster_topic_mapping = dict(zip(cluster_to_topic['cluster'], cluster_to_topic['topic_id']))\n",
    "\n",
    "# Assign the matched topic_id back to the DataFrame\n",
    "df_train_label['matched_topic_id'] = df_train_label['cluster'].map(cluster_topic_mapping)\n",
    "\n",
    "# Check the mapping\n",
    "print(df_train_label[['quote_text', 'cluster', 'topic_id', 'matched_topic_id']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "94aa9db9-e719-414c-aece-c1f6befb8ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict for df_test\n",
    "df_test_label['data_word2vec_embedding'] = df_test_label['data_word2vec_embedding'].apply(lambda x: np.fromstring(x.strip(\"[]\"), sep=\" \") if isinstance(x, str) else np.array(x))\n",
    "X = np.vstack(df_test_label['data_word2vec_embedding'].values)#.astype(np.float64)\n",
    "df_test_label['cluster'] = kmeans.predict(X)\n",
    "# map to the topic id\n",
    "df_test_label['matched_topic_id'] = df_test_label['cluster'].map(cluster_topic_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a5daa9c4-3275-4e2f-8a04-b18773fb8e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.2907732921254266\n",
      "test accuracy: 0.28689157867628123\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = np.sum(df_train_label['matched_topic_id'] == df_train_label['topic_id'])/len(df_train_label)\n",
    "test_accuracy = np.sum(df_test_label['matched_topic_id'] == df_test_label['topic_id'])/len(df_test_label)\n",
    "print('train accuracy:', train_accuracy)\n",
    "print('test accuracy:', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827b155c-1e45-4cf0-92d8-7817933db52b",
   "metadata": {},
   "source": [
    "**tune the pretrained model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "645b7cec-8362-43bd-b9fe-d8a899ef73a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Convert KeyedVectors to a trainable Word2Vec model\n",
    "word2vec_model = Word2Vec(vector_size=300, window=5, min_count=1)  # Keep same vector size\n",
    "word2vec_model.build_vocab_from_freq(model.key_to_index)  # Load pretrained vocab\n",
    "word2vec_model.build_vocab(df_train_label['tokenized_text'], update=True)  # Update vocab with new sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9a036b16-bd1a-4156-aa74-b667f7c792cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1204061, 1707595)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this rate can be tuned\n",
    "word2vec_model.train(df_train_label['tokenized_text'], total_examples=len(df_train_label['tokenized_text']), epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9ba4fc9b-34e4-4c31-86cb-6186e0bc61b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert fine-tuned Word2Vec model to KeyedVectors\n",
    "fine_tuned_keyed_vectors = word2vec_model.wv\n",
    "\n",
    "df_train_label[\"fine_word2vec_embedding\"] = df_train_label[\"tokenized_text\"].apply(lambda tokens: get_word2vec_embedding(tokens, fine_tuned_keyed_vectors))\n",
    "df_test_label[\"fine_word2vec_embedding\"] = df_test_label[\"tokenized_text\"].apply(lambda tokens: get_word2vec_embedding(tokens, fine_tuned_keyed_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2789a694-0286-4e54-882c-d57458a04da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.20198313051316721\n",
      "test accuracy: 0.1916044295647695\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = np.sum(df_train_label['matched_topic_id'] == df_train_label['topic_id'])/len(df_train_label)\n",
    "test_accuracy = np.sum(df_test_label['matched_topic_id'] == df_test_label['topic_id'])/len(df_test_label)\n",
    "print('train accuracy:', train_accuracy)\n",
    "print('test accuracy:', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396ad309-fd9b-408a-91d4-f80f31d26c0a",
   "metadata": {},
   "source": [
    "**what if we just use the one trained by data?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0e5f9005-9ac7-4919-baa3-63aa089a4f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_w2v_model = Word2Vec(sentences=df_train_label['tokenized_text'], vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6d0f5399-5eeb-47e8-99d5-e6cfa3e56fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_keyed_vectors = data_w2v_model.wv\n",
    "\n",
    "df_train_label[\"data_word2vec_embedding\"] = df_train_label[\"tokenized_text\"].apply(lambda tokens: get_word2vec_embedding(tokens, data_keyed_vectors))\n",
    "df_test_label[\"data_word2vec_embedding\"] = df_test_label[\"tokenized_text\"].apply(lambda tokens: get_word2vec_embedding(tokens, data_keyed_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0f8b4281-ba1d-4da0-aaed-e6ffdd6f44ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.3167213959178417\n",
      "test accuracy: 0.3139325263971156\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = np.sum(df_train_label['matched_topic_id'] == df_train_label['topic_id'])/len(df_train_label)\n",
    "test_accuracy = np.sum(df_test_label['matched_topic_id'] == df_test_label['topic_id'])/len(df_test_label)\n",
    "print('train accuracy:', train_accuracy)\n",
    "print('test accuracy:', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fe656f-a030-4e63-bd44-25c066645062",
   "metadata": {},
   "source": [
    "**let's try by tuning it with the full data set, perform classification on the labeled ones**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4f0d1836-f472-451c-b2b4-4403deca9622",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"dataset.parquet\")\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.2,random_state=42, stratify=df['label'] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7306e31-9fe2-4fbc-a902-9b8707c96ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process for df_train\n",
    "processed_texts = []\n",
    "# Iterate through the DataFrame rows\n",
    "for index, row in df_train.iterrows():\n",
    "    processed_text = process_text_pipeline(row['quote_text'], row['country_name'])\n",
    "    processed_texts.append(processed_text)\n",
    "\n",
    "# Assign lists back to the DataFrame\n",
    "df_train['processed_text_nsc'] = processed_texts\n",
    "\n",
    "# Repeat for df_test\n",
    "processed_texts = []\n",
    "\n",
    "for index, row in df_test.iterrows():\n",
    "    processed_text = process_text_pipeline(row['quote_text'], row['country_name'])\n",
    "    processed_texts.append(processed_text)\n",
    "\n",
    "df_test['processed_text_nsc'] = processed_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "35f67993-64fa-4159-82e3-b0afb01c9ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply tokenization based on country_name\n",
    "df_train['tokenized_text'] = df_train.apply(\n",
    "    lambda row: spacy_tokenize(row['processed_text_nsc'], 'fr' if row['country_name'] == 'France' else 'en'),\n",
    "    axis=1\n",
    ")\n",
    "df_test['tokenized_text'] = df_test.apply(\n",
    "    lambda row: spacy_tokenize(row['processed_text_nsc'], 'fr' if row['country_name'] == 'France' else 'en'),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "308a7ca4-270a-4326-8fd9-f963fc2120cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_w2v_model = Word2Vec(sentences=df_train['tokenized_text'], vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b92b78f2-a9f5-4bde-bdcc-19a3df1a745f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19669/2185601123.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train_label[\"data_word2vec_embedding\"] = df_train_label[\"tokenized_text\"].apply(lambda tokens: get_word2vec_embedding(tokens, data_keyed_vectors))\n",
      "/tmp/ipykernel_19669/2185601123.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test_label[\"data_word2vec_embedding\"] = df_test_label[\"tokenized_text\"].apply(lambda tokens: get_word2vec_embedding(tokens, data_keyed_vectors))\n"
     ]
    }
   ],
   "source": [
    "data_keyed_vectors = data_w2v_model.wv\n",
    "\n",
    "df_train_label = df_train[df_train['label'] == True]\n",
    "df_test_label = df_test[df_test['label'] == True]\n",
    "\n",
    "df_train_label[\"data_word2vec_embedding\"] = df_train_label[\"tokenized_text\"].apply(lambda tokens: get_word2vec_embedding(tokens, data_keyed_vectors))\n",
    "df_test_label[\"data_word2vec_embedding\"] = df_test_label[\"tokenized_text\"].apply(lambda tokens: get_word2vec_embedding(tokens, data_keyed_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0703ad95-a8ad-4e78-845a-0d9a9547cf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "df_train_label['fasttext_embedding'] = df_train_label['fasttext_embedding'].apply(lambda x: np.fromstring(x.strip(\"[]\"), sep=\" \") if isinstance(x, str) else np.array(x))\n",
    "# Stack embeddings properly\n",
    "X = np.vstack(df_train_label['fasttext_embedding'].values)\n",
    "\n",
    "# Apply K-Means clustering\n",
    "k = 11  # Number of clusters, 10 topics add out of topic\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "df_train_label['cluster'] = kmeans.fit_predict(X)\n",
    "\n",
    "\n",
    "# Group by cluster and topic_id to see distribution\n",
    "cluster_topic_distribution = df_train_label.groupby(['cluster', 'topic_id']).size().reset_index(name='count')\n",
    "\n",
    "# Find the most common topic_id in each cluster\n",
    "cluster_to_topic = cluster_topic_distribution.sort_values('count', ascending=False).drop_duplicates('cluster')\n",
    "\n",
    "# Create a mapping from cluster to topic_id\n",
    "cluster_topic_mapping = dict(zip(cluster_to_topic['cluster'], cluster_to_topic['topic_id']))\n",
    "\n",
    "# Assign the matched topic_id back to the DataFrame\n",
    "df_train_label['matched_topic_id'] = df_train_label['cluster'].map(cluster_topic_mapping)\n",
    "\n",
    "# predict for df_test\n",
    "df_test_label['fasttext_embedding'] = df_test_label['fasttext_embedding'].apply(lambda x: np.fromstring(x.strip(\"[]\"), sep=\" \") if isinstance(x, str) else np.array(x))\n",
    "X = np.vstack(df_test_label['fasttext_embedding'].values)#.astype(np.float64)\n",
    "df_test_label['cluster'] = kmeans.predict(X)\n",
    "# map to the topic id\n",
    "df_test_label['matched_topic_id'] = df_test_label['cluster'].map(cluster_topic_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e09ecbf8-f5eb-4697-ab69-029a2bf404e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.3141459017448973\n",
      "test accuracy: 0.31496265773886173\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = np.sum(df_train_label['matched_topic_id'] == df_train_label['topic_id'])/len(df_train_label)\n",
    "test_accuracy = np.sum(df_test_label['matched_topic_id'] == df_test_label['topic_id'])/len(df_test_label)\n",
    "print('train accuracy:', train_accuracy)\n",
    "print('test accuracy:', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c30255b-e4f9-46af-820e-4a3bd2b45810",
   "metadata": {},
   "source": [
    "**try fast text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cadef4dd-8597-4436-8735-82b2fe2a0de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "fasttext_model = FastText(sentences=df_train_label['processed_text_nsc'], vector_size=300, window=5, min_count=5, workers=4, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f1f0c778-6ae0-4367-b991-351461d438a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19669/1199099426.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train_label[\"fasttext_embedding\"] = df_train_label[\"tokenized_text\"].apply(lambda tokens: get_word2vec_embedding(tokens, fasttext_vectors))\n",
      "/tmp/ipykernel_19669/1199099426.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test_label[\"fasttext_embedding\"] = df_test_label[\"tokenized_text\"].apply(lambda tokens: get_word2vec_embedding(tokens, fasttext_vectors))\n"
     ]
    }
   ],
   "source": [
    "fasttext_vectors = fasttext_model.wv\n",
    "\n",
    "df_train_label[\"fasttext_embedding\"] = df_train_label[\"tokenized_text\"].apply(lambda tokens: get_word2vec_embedding(tokens, fasttext_vectors))\n",
    "df_test_label[\"fasttext_embedding\"] = df_test_label[\"tokenized_text\"].apply(lambda tokens: get_word2vec_embedding(tokens, fasttext_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9cf16cb8-d2cc-43ce-8671-620b077c972e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.2061039211898783\n",
      "test accuracy: 0.2142673190831831\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = np.sum(df_train_label['matched_topic_id'] == df_train_label['topic_id'])/len(df_train_label)\n",
    "test_accuracy = np.sum(df_test_label['matched_topic_id'] == df_test_label['topic_id'])/len(df_test_label)\n",
    "print('train accuracy:', train_accuracy)\n",
    "print('test accuracy:', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b36caca-70f6-45b7-b560-d75ff9411959",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
