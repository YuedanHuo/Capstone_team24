{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a863c0cf-5d8d-4627-bffb-de69f965cc21",
   "metadata": {},
   "source": [
    "**Train test split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "617392d0-312a-43f4-9f0e-4d246d9a8302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (88911, 10)\n",
      "Test set size: (22228, 10)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#load the data\n",
    "df = pd.read_parquet(\"dataset.parquet\")\n",
    "\n",
    "# do train-test split\n",
    "# since the distribution of languages, labels and topics are quite uniform\n",
    "# I just do simple train-test split for now\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Perform train-test split on the full DataFrame\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n",
    "\n",
    "# Check dataset sizes\n",
    "print(f\"Training set size: {df_train.shape}\")\n",
    "print(f\"Test set size: {df_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8259ffd9-a0f2-4228-a8f7-5cdd0925b01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution in training set:\n",
      "label\n",
      "False    0.82532\n",
      "True     0.17468\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Label distribution in test set:\n",
      "label\n",
      "False    0.82531\n",
      "True     0.17469\n",
      "Name: proportion, dtype: float64\n",
      "Language distribution in training set:\n",
      "country_name\n",
      "United Kingdom    0.510567\n",
      "France            0.489433\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Language distribution in test set:\n",
      "country_name\n",
      "United Kingdom    0.513227\n",
      "France            0.486773\n",
      "Name: proportion, dtype: float64\n",
      "Topic distribution in training set:\n",
      "topic_id\n",
      "554.0    0.103215\n",
      "602.0    0.103001\n",
      "547.0    0.102765\n",
      "544.0    0.102401\n",
      "546.0    0.102144\n",
      "550.0    0.102101\n",
      "552.0    0.101930\n",
      "543.0    0.101544\n",
      "556.0    0.101330\n",
      "600.0    0.079569\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Topic distribution in test set:\n",
      "topic_id\n",
      "543.0    0.106677\n",
      "552.0    0.105152\n",
      "550.0    0.104474\n",
      "546.0    0.104304\n",
      "544.0    0.103288\n",
      "547.0    0.101847\n",
      "602.0    0.100915\n",
      "554.0    0.100068\n",
      "556.0    0.095238\n",
      "600.0    0.078038\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# check the distribution on each data set\n",
    "print(\"Label distribution in training set:\")\n",
    "print(df_train['label'].value_counts(normalize=True))\n",
    "\n",
    "print(\"\\nLabel distribution in test set:\")\n",
    "print(df_test['label'].value_counts(normalize=True))\n",
    "\n",
    "print(\"Language distribution in training set:\")\n",
    "print(df_train['country_name'].value_counts(normalize=True))\n",
    "\n",
    "print(\"\\nLanguage distribution in test set:\")\n",
    "print(df_test['country_name'].value_counts(normalize=True))\n",
    "\n",
    "print(\"Topic distribution in training set:\")\n",
    "print(df_train['topic_id'].value_counts(normalize=True))\n",
    "\n",
    "print(\"\\nTopic distribution in test set:\")\n",
    "print(df_test['topic_id'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69d39e0-6bcf-47a8-a19d-e071bf1d501b",
   "metadata": {},
   "source": [
    "**clean the text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "01137541-52e5-4a30-a29f-d50b6fc34d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check null and replace them with empty string\n",
    "# and also strip the text\n",
    "def clean_text(text):\n",
    "    text = str(text).strip() if pd.notna(text) else \"\"  # Handle NaN & strip spaces\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b41ab14-ee18-4974-903b-ae6fabe8982e",
   "metadata": {},
   "source": [
    "**correct wrong spelling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360e8b28-5de8-4bb6-8106-1d8a4ba0fd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d4d4400-8cbc-4d90-adde-d4af938931c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country: France\n",
      "Original : Idéal pour créer une variété de coiffures sans tirer ni abîmer les cheveux\n",
      "Corrected: Idéal pour créer une variété de coiffures sans tirer ni abîmer les cheveux\n",
      "--------------------------------------------------\n",
      "Country: United Kingdom\n",
      "Original : but i found that this one was SO much better!!\n",
      "Corrected: but i found that this one was SO much better!!\n",
      "--------------------------------------------------\n",
      "Country: United Kingdom\n",
      "Original : It makes it ooze as soon as you uncap it and it makes a mess everywhere.\n",
      "Corrected: It makes it ooze as soon as you uncap it and it makes a mess everywhere.\n",
      "--------------------------------------------------\n",
      "Country: United Kingdom\n",
      "Original : I am in love with the beauty light wands and this one is no different it’s the perfect highlight shade and gives the most gorgeous glow and pillow talk hue to the skin!!\n",
      "Corrected: I am in love with the beauty light wands and this one is no different it’s the perfect highlight shade and gives the most gorgeous glow and pillow talk hue to the skin!!\n",
      "--------------------------------------------------\n",
      "Country: France\n",
      "Original : Je ne regrette pas du tout mon achat.\n",
      "Corrected: Je ne regrette pas du tout mon achat.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "# Load spell checkers for English and French\n",
    "spell_en = SpellChecker(language=\"en\")\n",
    "spell_fr = SpellChecker(language=\"fr\")\n",
    "\n",
    "# function to correct spelling\n",
    "def correct_tokens(tokens, spell):\n",
    "    \"\"\"\n",
    "    Corrects spelling of word tokens while keeping punctuation untouched.\n",
    "    \"\"\"\n",
    "    corrected_tokens = []\n",
    "    for token in tokens:\n",
    "        if token.isalpha():  # Only check spelling for words\n",
    "            corrected_word = spell.correction(token)\n",
    "            corrected_tokens.append(corrected_word if corrected_word else token)  # Keep original if no correction\n",
    "        else:\n",
    "            corrected_tokens.append(token)  # Leave punctuation untouched\n",
    "    return corrected_tokens\n",
    "\n",
    "# function to assign spellcheck according to country\n",
    "def correct_spelling(text, country):\n",
    "    spell = spell_fr if country == 'France' else spell_en  # Choose correct spell checker\n",
    "    \n",
    "    tokens = text.split()  # Tokenize text\n",
    "    corrected_tokens = correct_tokens(tokens, spell)  # Apply spell checking\n",
    "    return \" \".join(corrected_tokens)  # Convert back to string\n",
    "\n",
    "# Test with sample rows\n",
    "sample_rows = df.sample(n=5, random_state=42)\n",
    "\n",
    "for _, row in sample_rows.iterrows():\n",
    "    original_text = row['quote_text']\n",
    "    country = row['country_name']\n",
    "    corrected_text = correct_spelling(original_text, country)\n",
    "\n",
    "    print(f\"Country: {country}\")\n",
    "    print(f\"Original : {original_text}\")\n",
    "    print(f\"Corrected: {corrected_text}\")\n",
    "    print(\"-\" * 50)  # Separator for readability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0273a09-2db7-470b-8a28-5a29e87fb58a",
   "metadata": {},
   "source": [
    "**replace emoji with the corresponding meaning in each language**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bd83a5-ea11-4d73-993d-4a5179a82fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45164c48-37cb-4902-aa46-b95250c75df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country: France\n",
      "Original : - Flacon à la contenance généreuse.\n",
      "With Emoji Text: - Flacon à la contenance généreuse.\n",
      "--------------------------------------------------\n",
      "Country: France\n",
      "Original : Je conseille ce produit pour les peaux très sèches et fragiles.\n",
      "With Emoji Text: Je conseille ce produit pour les peaux très sèches et fragiles.\n",
      "--------------------------------------------------\n",
      "Country: United Kingdom\n",
      "Original : I have shaken the moisturiser vigorously before use, and it makes a clicking sound, suggesting there might be a small sphere inside for mixing.\n",
      "With Emoji Text: I have shaken the moisturiser vigorously before use, and it makes a clicking sound, suggesting there might be a small sphere inside for mixing.\n",
      "--------------------------------------------------\n",
      "Country: United Kingdom\n",
      "Original : I have no complaints about this product.\n",
      "With Emoji Text: I have no complaints about this product.\n",
      "--------------------------------------------------\n",
      "Country: United Kingdom\n",
      "Original : Highly recommend this product.I also love the scent and the unique packaging.\n",
      "With Emoji Text: Highly recommend this product.I also love the scent and the unique packaging.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import emoji\n",
    "\n",
    "# Function to replace emoji with text based on country\n",
    "def replace_emoji(text, country):\n",
    "    lang = \"fr\" if country == \"France\" else \"en\"  # Choose French or English\n",
    "    return emoji.demojize(text, language=lang).replace(\":\", \"\").replace(\"_\", \" \")  # Clean up output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67e702cc-07e7-4469-ba35-b3fd1a4a765f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               quote_text\n",
      "22      I really loved the packaging of this product i...\n",
      "108     The bottle is so pretty 😍 it's got a bit of a ...\n",
      "233     🥰❤️It has a unique aroma, I love it, and it is...\n",
      "445     but compared to my other selection of aftersha...\n",
      "1287    From the intriguing  packaging topped with a r...\n",
      "...                                                   ...\n",
      "110763                       ❤️ 5 starts for me for sure!\n",
      "110860  Brosse arrivée cassée, à la base de la brosse ...\n",
      "110903                    Et notre coiffeuse en a aussi 🤩\n",
      "110958                                Just as described 😎\n",
      "110987                     very fast delivery excellent 😊\n",
      "\n",
      "[1049 rows x 1 columns]\n",
      "Country: United Kingdom\n",
      "Original : I really loved the packaging of this product it's unique and beautiful 😍.\n",
      "With Emoji Text: I really loved the packaging of this product it's unique and beautiful smiling face with heart-eyes.\n",
      "--------------------------------------------------\n",
      "Country: United Kingdom\n",
      "Original : The bottle is so pretty 😍 it's got a bit of a curve to it so it stands out and the cap has some jelly material flowers that are so cute.\n",
      "With Emoji Text: The bottle is so pretty smiling face with heart-eyes it's got a bit of a curve to it so it stands out and the cap has some jelly material flowers that are so cute.\n",
      "--------------------------------------------------\n",
      "Country: United Kingdom\n",
      "Original : 🥰❤️It has a unique aroma, I love it, and it is very effective with its ingredients.\n",
      "With Emoji Text: smiling face with heartsred heartIt has a unique aroma, I love it, and it is very effective with its ingredients.\n",
      "--------------------------------------------------\n",
      "Country: United Kingdom\n",
      "Original : but compared to my other selection of aftershaves it's not strong enough to stand out for me, bought it about 2 weeks ago and used it twice, looks like one of my friends is getting a free full bottle of aftershave for free 😞\n",
      "With Emoji Text: but compared to my other selection of aftershaves it's not strong enough to stand out for me, bought it about 2 weeks ago and used it twice, looks like one of my friends is getting a free full bottle of aftershave for free disappointed face\n",
      "--------------------------------------------------\n",
      "Country: United Kingdom\n",
      "Original : From the intriguing  packaging topped with a raised 🧲 Magnet shape, to the unique Extender zig-zag bristle brush - it delivers a unique experience while deftly adding lift and realistic volume.\n",
      "With Emoji Text: From the intriguing  packaging topped with a raised magnet Magnet shape, to the unique Extender zig-zag bristle brush - it delivers a unique experience while deftly adding lift and realistic volume.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Function to check if a text contains emojis\n",
    "def contains_emoji(text):\n",
    "    text = str(text)  # Convert to string to avoid errors\n",
    "    return any(char in emoji.EMOJI_DATA for char in text)\n",
    "\n",
    "# Print rows containing emojis\n",
    "emoji_rows = df[df['quote_text'].apply(contains_emoji)]\n",
    "print(emoji_rows[['quote_text']])\n",
    "\n",
    "# try to see if we have replace them\n",
    "for _, row in emoji_rows.head().iterrows():\n",
    "    original_text = row['quote_text']\n",
    "    country = row['country_name']\n",
    "    replaced_text = replace_emoji(original_text, country)\n",
    "\n",
    "    print(f\"Country: {country}\")\n",
    "    print(f\"Original : {original_text}\")\n",
    "    print(f\"With Emoji Text: {replaced_text}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f48134e-7dfd-4f57-b2ca-db52eb066d5c",
   "metadata": {},
   "source": [
    "**lemmatization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442e5256-d062-4801-bea7-12acd1367af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "22c35504-23e2-4564-82d4-3c8fd9c5b9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "Collecting fr-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.8.0/fr_core_news_sm-3.8.0-py3-none-any.whl (16.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl\n",
    "!pip install https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.8.0/fr_core_news_sm-3.8.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ca1ea899-7f2b-4883-a490-7d7f13166d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country: France\n",
      "Original : Idéal pour créer une variété de coiffures sans tirer ni abîmer les cheveux\n",
      "Lemmatized: idéal pour créer un variété de coiffure sans tirer ni abîmer le cheveu\n",
      "--------------------------------------------------\n",
      "Country: United Kingdom\n",
      "Original : but i found that this one was SO much better!!\n",
      "Lemmatized: but I find that this one be so much well ! !\n",
      "--------------------------------------------------\n",
      "Country: United Kingdom\n",
      "Original : It makes it ooze as soon as you uncap it and it makes a mess everywhere.\n",
      "Lemmatized: it make it ooze as soon as you uncap it and it make a mess everywhere .\n",
      "--------------------------------------------------\n",
      "Country: United Kingdom\n",
      "Original : I am in love with the beauty light wands and this one is no different it’s the perfect highlight shade and gives the most gorgeous glow and pillow talk hue to the skin!!\n",
      "Lemmatized: I be in love with the beauty light wand and this one be no different it ’ the perfect highlight shade and give the most gorgeous glow and pillow talk hue to the skin ! !\n",
      "--------------------------------------------------\n",
      "Country: France\n",
      "Original : Je ne regrette pas du tout mon achat.\n",
      "Lemmatized: je ne regretter pas de tout mon achat .\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load English & French models\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "nlp_fr = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "# Function to lemmatize text based on country\n",
    "def lemmatize_text(text, country):\n",
    "    nlp = nlp_fr if country == \"France\" else nlp_en  # Choose model\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.lemma_ for token in doc])  # Get lemmatized words\n",
    "\n",
    "# Select a few random samples\n",
    "sample_rows = df.sample(n=5, random_state=42)\n",
    "\n",
    "for _, row in sample_rows.iterrows():\n",
    "    original_text = row['quote_text']\n",
    "    country = row['country_name']\n",
    "    lemmatized_text = lemmatize_text(original_text, country)\n",
    "\n",
    "    print(f\"Country: {country}\")\n",
    "    print(f\"Original : {original_text}\")\n",
    "    print(f\"Lemmatized: {lemmatized_text}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621ca5fd-f5b8-477f-aeb3-cf482e824954",
   "metadata": {},
   "source": [
    "**create a pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0a5950e6-e8e2-423e-b0dd-2e72f1dc9f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_pipeline(text, country):\n",
    "    text = clean_text(text)\n",
    "    text = correct_spelling(text, country)\n",
    "    text = replace_emoji(text, country)\n",
    "    text = lemmatize_text(text, country)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bed862d-9ec7-4311-9dcd-4f9e7bf3ad42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply pipeline to df_train\n",
    "df_train[['processed_text', 'embedding']] = df_train.apply(\n",
    "    lambda row: process_text_pipeline(row['quote_text'], row['country_name']), axis=1, result_type=\"expand\"\n",
    ")\n",
    "\n",
    "# Apply pipeline to df_test\n",
    "df_test[['processed_text', 'embedding']] = df_test.apply(\n",
    "    lambda row: process_text_pipeline(row['quote_text'], row['country_name']), axis=1, result_type=\"expand\"\n",
    ")\n",
    "\n",
    "# Display sample results\n",
    "print(\"Training set sample:\")\n",
    "print(df_train[['quote_text', 'processed_text', 'embedding']].head())\n",
    "\n",
    "print(\"\\nTest set sample:\")\n",
    "print(df_test[['quote_text', 'processed_text', 'embedding']].head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
