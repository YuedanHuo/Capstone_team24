{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebe429b6-8c32-4f86-8dad-0196011bb4f2",
   "metadata": {},
   "source": [
    "# Online Fine-Tuning of a BERT Model for Continual Classification\n",
    "\n",
    "## Introduction\n",
    "In dynamic environments where data is continuously generated (e.g., daily quotes), an online or continual learning approach enables a pre-trained BERT model to be incrementally fine-tuned. This method allows the model to adapt to evolving language patterns and data distributions while retaining previously learned knowledge.\n",
    "\n",
    "## Methodology\n",
    "1. **Initial Fine-Tuning:**\n",
    "   - Start with a pre-trained BERT model.\n",
    "   - Fine-tune it on the initial dataset for the target classification task (binary or multiclass).\n",
    "\n",
    "2. **Continual Updates:**\n",
    "   - As new data arrives (e.g., daily quotes), periodically fine-tune the existing model using the new data.\n",
    "   - Fine-tuning is done in batches (e.g., daily or weekly) rather than on a per-sample basis.\n",
    "   - Optionally, mix new data with a subset of historical data to preserve prior knowledge.\n",
    "\n",
    "3. **Pipeline Setup:**\n",
    "   - Establish a regular schedule for model updates and performance evaluation.\n",
    "   - Continue training from the current state of the model using transfer learning principles.\n",
    "\n",
    "## Considerations\n",
    "- **Catastrophic Forgetting:**\n",
    "  - *Problem:* Fine-tuning on new data can lead the model to forget previously learned information.\n",
    "  - *Mitigation Strategies:*\n",
    "    - **Rehearsal:** Include a subset of historical data in each update.\n",
    "    - **Regularization:** Use methods like Elastic Weight Consolidation (EWC) to prevent drastic changes in important parameters.\n",
    "      - Reference: [Kirkpatrick et al., 2017](https://www.pnas.org/doi/10.1073/pnas.1611835114)\n",
    "    - **Memory-Based Approaches:** Maintain a small buffer of past examples to be revisited during fine-tuning.\n",
    "\n",
    "- **Hyperparameter Tuning:**\n",
    "  - Adjust learning rates, batch sizes, and the number of epochs based on the new data.\n",
    "  - Monitor performance on both recent and historical validation sets to ensure balanced learning.\n",
    "\n",
    "- **Data Distribution Shifts:**\n",
    "  - Continuously monitor for changes in data characteristics.\n",
    "  - Adapt the update strategy if significant shifts in the distribution are detected.\n",
    "\n",
    "- **Computational Resources:**\n",
    "  - Online fine-tuning can be resource-intensive.\n",
    "  - Optimize update frequency and batch sizes to maintain a balance between performance improvements and resource usage.\n",
    "\n",
    "## Conclusion\n",
    "Online fine-tuning of a BERT model is an effective strategy for maintaining up-to-date classifiers in environments with continuous data influx. By scheduling periodic updates and employing techniques to mitigate catastrophic forgetting, you can ensure that the model remains robust and performs well on both new and previously seen data.\n",
    "\n",
    "## References\n",
    "- Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A. A., ... & Hadsell, R. (2017). **Overcoming catastrophic forgetting in neural networks**. *Proceedings of the National Academy of Sciences, 114*(13), 3521-3526. [Link](https://www.pnas.org/doi/10.1073/pnas.1611835114)\n",
    "- Parisi, G. I., Kemker, R., Part, J. L., Kanan, C., & Wermter, S. (2019). **Continual lifelong learning with neural networks: A review**. *Neural Networks, 113*, 54-71. [Link](https://doi.org/10.1016/j.neunet.2019.01.012)\n",
    "- Hugging Face Transformers Documentation. [Link](https://huggingface.co/transformers/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "264ad330-d482-4c36-9985-a7858ad529c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_train = pd.read_csv('df_train_sbert.csv')\n",
    "df_test = pd.read_csv('df_test_sbert.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8d3beaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/anaconda3/lib/python3.12/site-packages (25.0)\n",
      "Collecting pip\n",
      "  Downloading pip-25.0.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Downloading pip-25.0.1-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 25.0\n",
      "    Uninstalling pip-25.0:\n",
      "      Successfully uninstalled pip-25.0\n",
      "Successfully installed pip-25.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88c57f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.48.3-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.24.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.28.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.5.2-cp38-abi3-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\n",
      "Downloading transformers-4.48.3-py3-none-any.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.28.1-py3-none-any.whl (464 kB)\n",
      "Downloading safetensors-0.5.2-cp38-abi3-macosx_11_0_arm64.whl (408 kB)\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-macosx_11_0_arm64.whl (2.6 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.28.1 safetensors-0.5.2 tokenizers-0.21.0 transformers-4.48.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4b8ec0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-keras\n",
      "  Downloading tf_keras-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: tensorflow<2.19,>=2.18 in /opt/anaconda3/lib/python3.12/site-packages (from tf-keras) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.68.1)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (3.7.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (3.11.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.4.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow<2.19,>=2.18->tf-keras) (0.44.0)\n",
      "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (13.7.1)\n",
      "Requirement already satisfied: namex in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: optree in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (0.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (0.1.0)\n",
      "Downloading tf_keras-2.18.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tf-keras\n",
      "Successfully installed tf-keras-2.18.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tf-keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fabce76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.3.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (4.66.5)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.10.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.11.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-3.3.0-py3-none-any.whl (484 kB)\n",
      "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading xxhash-3.5.0-cp312-cp312-macosx_11_0_arm64.whl (30 kB)\n",
      "Installing collected packages: xxhash, multiprocess, datasets\n",
      "Successfully installed datasets-3.3.0 multiprocess-0.70.16 xxhash-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24f703f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: transformers[torch]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "688e6e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate>=0.26.0\n",
      "  Downloading accelerate-1.3.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from accelerate>=0.26.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from accelerate>=0.26.0) (24.1)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/lib/python3.12/site-packages (from accelerate>=0.26.0) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /opt/anaconda3/lib/python3.12/site-packages (from accelerate>=0.26.0) (6.0.1)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from accelerate>=0.26.0) (2.6.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /opt/anaconda3/lib/python3.12/site-packages (from accelerate>=0.26.0) (0.28.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from accelerate>=0.26.0) (0.5.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2024.6.1)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.11.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate>=0.26.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate>=0.26.0) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2024.8.30)\n",
      "Downloading accelerate-1.3.0-py3-none-any.whl (336 kB)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-1.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install 'accelerate>=0.26.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf3f8d27-ad8f-4311-a7dc-57223de11bd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "379fef6d96ab4c5da5ec6f7a8f0e5d95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/88911 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b56e97351a24187ae275e9e8fed9b7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/22228 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/anaconda3/lib/python3.12/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16671' max='16671' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16671/16671 4:18:34, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.220500</td>\n",
       "      <td>0.141461</td>\n",
       "      <td>0.941560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.137300</td>\n",
       "      <td>0.146863</td>\n",
       "      <td>0.943990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.063100</td>\n",
       "      <td>0.152065</td>\n",
       "      <td>0.948488</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.152065247297287, 'eval_accuracy': 0.9484883930178154, 'eval_runtime': 372.3748, 'eval_samples_per_second': 59.693, 'eval_steps_per_second': 1.866, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "\n",
    "# Assume df_train and df_test already exist and have been preprocessed with:\n",
    "# - 'processed_text': the preprocessed text from your pipeline.\n",
    "# - 'label': binary classification label (0 for Out-of-Topic, 1 for In-Topic)\n",
    "\n",
    "\n",
    "# Convert the Pandas DataFrames to Hugging Face Datasets.\n",
    "train_dataset = Dataset.from_pandas(df_train)\n",
    "test_dataset  = Dataset.from_pandas(df_test)\n",
    "\n",
    "# Load the pre-trained BERT tokenizer.\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define tokenization function using the 'processed_text' column.\n",
    "def tokenize_function(examples):\n",
    "    texts = [str(text) for text in examples['processed_text']]\n",
    "    return tokenizer(texts, padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "\n",
    "# Tokenize the datasets.\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset  = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set format for PyTorch.\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# Initialize the BERT model for binary classification (num_labels=2).\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Define training arguments.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    ")\n",
    "\n",
    "# Define a simple accuracy metric.\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = np.mean(predictions == labels)\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "# Initialize the Trainer.\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Fine-tune the binary classification model.\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model.\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"Evaluation results:\", eval_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e68fbe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [[ 4.7244315 -4.2403507]\n",
      " [ 4.7080913 -4.238422 ]\n",
      " [ 4.4483476 -4.126842 ]\n",
      " ...\n",
      " [-1.6329443  1.8176602]\n",
      " [ 4.737137  -4.2433586]\n",
      " [ 4.6952157 -4.232057 ]]\n",
      "Labels: [0 0 0 ... 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "predictions_output = trainer.predict(test_dataset)\n",
    "print(\"Predictions:\", predictions_output.predictions)\n",
    "print(\"Labels:\", predictions_output.label_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abe82010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97     18301\n",
      "           1       0.82      0.90      0.86      3927\n",
      "\n",
      "    accuracy                           0.95     22228\n",
      "   macro avg       0.90      0.93      0.91     22228\n",
      "weighted avg       0.95      0.95      0.95     22228\n",
      "\n",
      "Confusion Matrix:\n",
      "[[17548   753]\n",
      " [  392  3535]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Assume predictions_output is the result from trainer.predict(test_dataset)\n",
    "# For models that output logits, use argmax to convert to predicted labels.\n",
    "predictions_output = trainer.predict(test_dataset)\n",
    "predicted_labels = np.argmax(predictions_output.predictions, axis=1)\n",
    "true_labels = predictions_output.label_ids\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(true_labels, predicted_labels))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(true_labels, predicted_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ccc70c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples predicted as in-topic: 16767\n",
      "Downstream training set shape: (16767, 14)\n",
      "Value counts for downstream topics:\n",
      "downstream_topic\n",
      "602.0    2617\n",
      "543.0    2327\n",
      "546.0    2261\n",
      "0.0      2181\n",
      "544.0    2132\n",
      "550.0    2062\n",
      "547.0    1490\n",
      "600.0     881\n",
      "554.0     359\n",
      "552.0     230\n",
      "556.0     227\n",
      "Name: count, dtype: int64\n",
      "Unique downstream topics mapping: {0.0: 0, 543.0: 1, 544.0: 2, 546.0: 3, 547.0: 4, 550.0: 5, 552.0: 6, 554.0: 7, 556.0: 8, 600.0: 9, 602.0: 10}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---- Step 1: Extract Predicted Positives from Binary Classifier ----\n",
    "X_train_bin = df_train[\"sbert_embedding\"]\n",
    "y_train_bin = df_train[\"label\"]\n",
    "# - binary_predict is defined from the upstream task\n",
    "\n",
    "# Get binary predictions on the training set\n",
    "# from the upstream task\n",
    "binary_preds_output = trainer.predict(train_dataset)\n",
    "binary_preds_train = np.argmax(binary_preds_output.predictions, axis=1) # array of 0s and 1s\n",
    "\n",
    "# Find indices where the binary classifier predicts positive (in-topic)\n",
    "positive_indices = np.where(binary_preds_train == 1)[0]\n",
    "\n",
    "print(\"Number of samples predicted as in-topic:\", len(positive_indices))\n",
    "\n",
    "# ---- Step 2: Build a New Training Set for the Downstream Classifier ----\n",
    "# We'll extract rows from df_train corresponding to predicted positives.\n",
    "# Then, for each extracted sample:\n",
    "#   - If the true binary label is 1 (i.e., it is a true positive), keep its original 'topic_id'\n",
    "#   - If the true binary label is 0 (i.e., a false positive), set its 'topic_id' to \"NP\"\n",
    "\n",
    "df_downstream = df_train.iloc[positive_indices].copy()\n",
    "\n",
    "# Create a new column for the downstream topic label:\n",
    "df_downstream['downstream_topic'] = df_downstream.apply(\n",
    "    lambda row: row['topic_id'] if row['label'] == 1 else 0.0, axis=1\n",
    ")\n",
    "\n",
    "# Now, df_downstream contains only the samples predicted as in-topic.\n",
    "# Their 'downstream_topic' column holds the original topic for true positives,\n",
    "# and \"NP\" for false positives.\n",
    "\n",
    "print(\"Downstream training set shape:\", df_downstream.shape)\n",
    "print(\"Value counts for downstream topics:\")\n",
    "print(df_downstream['downstream_topic'].value_counts())\n",
    "\n",
    "# ---- Step 3: (Optional) Prepare Data for Downstream BERT Fine-Tuning ----\n",
    "# For instance, if you want to fine-tune a BERT classifier on this subset:\n",
    "# Make sure your downstream training set contains:\n",
    "# - 'processed_text': the input text.\n",
    "# - 'downstream_topic': the new multiclass labels (including \"NP\").\n",
    "\n",
    "# You might need to remap 'downstream_topic' to contiguous integers, for example:\n",
    "unique_topics = np.sort(df_downstream['downstream_topic'].unique())\n",
    "topic_mapping = {topic: idx for idx, topic in enumerate(unique_topics)}\n",
    "df_downstream['mapped_topic'] = df_downstream['downstream_topic'].map(topic_mapping)\n",
    "\n",
    "print(\"Unique downstream topics mapping:\", topic_mapping)\n",
    "\n",
    "# At this point, you can use df_downstream to train your downstream classifier.\n",
    "# For example, you could convert it to a Hugging Face Dataset and fine-tune a BERT model:\n",
    "from datasets import Dataset\n",
    "downstream_dataset = Dataset.from_pandas(df_downstream)\n",
    "\n",
    "# Then tokenize using your usual tokenization function,\n",
    "# and fine-tune a BERT model (or DistilBERT, etc.) on 'processed_text' with labels 'mapped_topic'.\n",
    "\n",
    "# (Fine-tuning code for BERT on downstream_dataset would go here.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c001694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['Unnamed: 0', 'country_id', 'country_name', 'product_id', 'product_category', 'review_id', 'review_text', 'quote_text', 'quote_id', 'topic_id', 'label', 'processed_text', 'sbert_embedding', 'downstream_topic', 'mapped_topic', '__index_level_0__'],\n",
      "    num_rows: 16767\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(downstream_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c44fce28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test samples predicted as in-topic: 4288\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7abbb896a45348fe9acc3b0614a19f26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16767 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fcd3ebad9714527b56d8a8635a184df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4288 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of downstream classes: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/anaconda3/lib/python3.12/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3144' max='3144' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3144/3144 51:41, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.704900</td>\n",
       "      <td>0.789463</td>\n",
       "      <td>0.732743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.558700</td>\n",
       "      <td>0.676869</td>\n",
       "      <td>0.759095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.440100</td>\n",
       "      <td>0.677937</td>\n",
       "      <td>0.758629</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downstream Evaluation results: {'eval_loss': 0.6768686175346375, 'eval_accuracy': 0.7590951492537313, 'eval_runtime': 68.9176, 'eval_samples_per_second': 62.219, 'eval_steps_per_second': 1.944, 'epoch': 3.0}\n",
      "Downstream Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.16      0.25       753\n",
      "           1       0.77      0.92      0.84       561\n",
      "           2       0.79      0.51      0.62       499\n",
      "           3       0.78      0.92      0.84       529\n",
      "           4       0.87      0.97      0.92       404\n",
      "           5       0.68      0.96      0.80       502\n",
      "           6       0.77      0.91      0.83        58\n",
      "           7       0.79      0.87      0.83        78\n",
      "           8       0.79      0.95      0.86        40\n",
      "           9       0.85      0.99      0.92       211\n",
      "          10       0.76      0.98      0.86       653\n",
      "\n",
      "    accuracy                           0.76      4288\n",
      "   macro avg       0.76      0.83      0.78      4288\n",
      "weighted avg       0.73      0.76      0.72      4288\n",
      "\n",
      "Downstream Confusion Matrix:\n",
      "[[123  69  57 112  39 125  13  16   8  32 159]\n",
      " [ 12 516   3   2   7   4   0   0   0   1  16]\n",
      " [ 64  77 253  18   7  70   1   1   0   0   8]\n",
      " [ 11   2   2 485   0  21   0   0   1   0   7]\n",
      " [  3   1   2   1 393   2   0   0   0   1   1]\n",
      " [  5   7   1   3   4 480   1   0   0   1   0]\n",
      " [  2   0   0   1   0   1  53   0   0   1   0]\n",
      " [  5   0   0   0   0   0   0  68   0   0   5]\n",
      " [  2   0   0   0   0   0   0   0  38   0   0]\n",
      " [  0   1   1   0   0   0   0   0   0 209   0]\n",
      " [  5   1   0   3   4   0   1   1   1   0 637]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# -------------\n",
    "# ASSUMPTIONS:\n",
    "# - df_train and df_test are for the binary task and include:\n",
    "#      'sbert_embedding': SBERT embeddings (lists/arrays)\n",
    "#      'label': binary labels (0 for out-of-topic, 1 for in-topic)\n",
    "# - df_downstream is for the downstream (multiclass) task and includes:\n",
    "#      'sbert_embedding': SBERT embeddings (lists/arrays)\n",
    "#      'topic_id': the original topic label for in-topic samples; NaN for out-of-topic.\n",
    "#\n",
    "# - topic_mapping is a dictionary mapping the original topic IDs (plus the out-of-topic placeholder)\n",
    "#   to contiguous integers.\n",
    "# -------------\n",
    "\n",
    "# --- STEP 1: Extract downstream test set from df_test based on binary predictions ---\n",
    "\n",
    "# Get binary predictions on df_test using your binary model Trainer (assumed already trained).\n",
    "# This returns an object; we extract predictions and then take argmax to get 0/1.\n",
    "binary_predictions_output = trainer.predict(test_dataset)  # 'trainer' is your binary model Trainer\n",
    "binary_preds_test = np.argmax(binary_predictions_output.predictions, axis=1)\n",
    "\n",
    "# Find indices where the binary classifier predicts in-topic (1)\n",
    "positive_indices_test = np.where(binary_preds_test == 1)[0]\n",
    "print(\"Number of test samples predicted as in-topic:\", len(positive_indices_test))\n",
    "\n",
    "# Create downstream test DataFrame from df_test (for multiclass stage).\n",
    "df_downstream_test = df_test.iloc[positive_indices_test].copy()\n",
    "\n",
    "# Create a new column 'downstream_topic':\n",
    "# If the true binary label is 1, use the true 'topic_id'; otherwise, mark as \"NP\".\n",
    "df_downstream_test['downstream_topic'] = df_downstream_test.apply(\n",
    "    lambda row: row['topic_id'] if row['label'] == 1 else 0.0, axis=1\n",
    ")\n",
    "\n",
    "# Map the downstream_topic to contiguous integers using topic_mapping.\n",
    "# (Ensure that topic_mapping is defined; for example, you might have built it from df_downstream.)\n",
    "df_downstream_test['mapped_topic'] = df_downstream_test['downstream_topic'].map(topic_mapping)\n",
    "\n",
    "# Create a Hugging Face Dataset for the downstream test set.\n",
    "downstream_dataset_test = Dataset.from_pandas(df_downstream_test)\n",
    "\n",
    "# --- STEP 2: Prepare the Downstream Training Data ---\n",
    "# Assume df_downstream is already prepared similarly (it includes 'processed_text', 'downstream_topic', and 'mapped_topic')\n",
    "downstream_dataset = Dataset.from_pandas(df_downstream)\n",
    "\n",
    "# --- STEP 3: Tokenization ---\n",
    "# Load the BERT tokenizer.\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Ensure text is a string.\n",
    "    texts = [str(text) for text in examples[\"processed_text\"]]\n",
    "    return tokenizer(texts, padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# Tokenize training and test downstream datasets.\n",
    "downstream_train = downstream_dataset.map(tokenize_function, batched=True)\n",
    "downstream_test  = downstream_dataset_test.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set the format for PyTorch; include the label column ('mapped_topic').\n",
    "downstream_train.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"mapped_topic\"])\n",
    "downstream_test.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"mapped_topic\"])\n",
    "\n",
    "# Determine the number of downstream classes.\n",
    "num_downstream_classes = len(np.unique(df_downstream[\"mapped_topic\"].values))\n",
    "print(\"Number of downstream classes:\", num_downstream_classes)\n",
    "\n",
    "# Rename the label column to \"labels\" for Trainer compatibility.\n",
    "downstream_train = downstream_train.rename_column(\"mapped_topic\", \"labels\")\n",
    "downstream_test  = downstream_test.rename_column(\"mapped_topic\", \"labels\")\n",
    "\n",
    "# --- STEP 4: Downstream BERT Fine-Tuning ---\n",
    "model_downstream = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_downstream_classes)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_downstream\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs_downstream\",\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = np.mean(predictions == labels)\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "trainer_downstream = Trainer(\n",
    "    model=model_downstream,\n",
    "    args=training_args,\n",
    "    train_dataset=downstream_train,\n",
    "    eval_dataset=downstream_test,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Fine-tune the downstream BERT model.\n",
    "trainer_downstream.train()\n",
    "\n",
    "# Evaluate the downstream model.\n",
    "eval_results_downstream = trainer_downstream.evaluate()\n",
    "print(\"Downstream Evaluation results:\", eval_results_downstream)\n",
    "\n",
    "predictions_output = trainer_downstream.predict(downstream_test)\n",
    "predicted_labels = np.argmax(predictions_output.predictions, axis=1)\n",
    "true_labels = predictions_output.label_ids\n",
    "\n",
    "print(\"Downstream Classification Report:\")\n",
    "print(classification_report(true_labels, predicted_labels))\n",
    "print(\"Downstream Confusion Matrix:\")\n",
    "print(confusion_matrix(true_labels, predicted_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d71f562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./downstream_bert_model/tokenizer_config.json',\n",
       " './downstream_bert_model/special_tokens_map.json',\n",
       " './downstream_bert_model/vocab.txt',\n",
       " './downstream_bert_model/added_tokens.json',\n",
       " './downstream_bert_model/tokenizer.json')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the binary classifier model\n",
    "trainer.model.save_pretrained(\"./binary_bert_model\")\n",
    "\n",
    "# Save the tokenizer, same for downstream and upstream\n",
    "tokenizer.save_pretrained(\"./binary_bert_model\")\n",
    "\n",
    "# Save the downstream classifier model\n",
    "trainer_downstream.model.save_pretrained(\"./downstream_bert_model\")\n",
    "\n",
    "# Save the tokenizer (it's the same tokenizer, but good practice to save it again)\n",
    "tokenizer.save_pretrained(\"./downstream_bert_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdea9520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to reload\n",
    "from transformers import BertForSequenceClassification, BertTokenizerFast\n",
    "\n",
    "# Load the downstream classifier model\n",
    "downstream_model = BertForSequenceClassification.from_pretrained(\"./downstream_bert_model\")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"./downstream_bert_model\")\n",
    "\n",
    "\n",
    "# Load the binary classifier model\n",
    "binary_model = BertForSequenceClassification.from_pretrained(\"./binary_bert_model\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397f950d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5b263d4",
   "metadata": {},
   "source": [
    "# **Analysis of BERT-on-BERT Downstream Performance**\n",
    "Your **BERT-based binary model** has already demonstrated strong performance with **95% accuracy**, and now we are evaluating its **downstream BERT multiclass classifier**.\n",
    "\n",
    "## **1. Key Overall Metrics**\n",
    "- **Loss**: `0.6769` (acceptable, but could be improved)\n",
    "- **Accuracy**: **`76%`** (solid, similar to previous methods but with better F1-macro)\n",
    "- **Macro F1 Score**: **`0.78`** (better than the XGB/SVM pipeline, which was around `0.62‚Äì0.64`)\n",
    "- **Weighted F1 Score**: **`0.72`** (shows overall balance in classification)\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Breakdown of Precision, Recall, and F1**\n",
    "| Topic ID | Precision | Recall | F1-score | Support | Notes |\n",
    "|----------|-----------|--------|----------|---------|-------|\n",
    "| **0**  | **0.53** | **0.16** | **0.25** | 753 | Many false positives from binary stage end up here |\n",
    "| **1**  | **0.77** | **0.92** | **0.84** | 561 | Very high recall, good precision |\n",
    "| **2**  | **0.79** | **0.51** | **0.62** | 499 | Precision is good, but recall is low |\n",
    "| **3**  | **0.78** | **0.92** | **0.84** | 529 | Strong recall and F1 |\n",
    "| **4**  | **0.87** | **0.97** | **0.92** | 404 | Very high precision and recall |\n",
    "| **5**  | **0.68** | **0.96** | **0.80** | 502 | Recall is much higher than precision |\n",
    "| **6**  | **0.77** | **0.91** | **0.83** | 58  | Very high recall |\n",
    "| **7**  | **0.79** | **0.87** | **0.83** | 78  | Consistently strong performance |\n",
    "| **8**  | **0.79** | **0.95** | **0.86** | 40  | Excellent recall |\n",
    "| **9**  | **0.85** | **0.99** | **0.92** | 211 | Almost perfect recall |\n",
    "| **10** | **0.76** | **0.98** | **0.86** | 653 | Very strong overall |\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Observations from Confusion Matrix**\n",
    "1. **Class 0 is still problematic** (False Positives from Binary Stage)\n",
    "   - **Recall = 16%**, meaning **many misclassified out-of-topic quotes are still incorrectly labeled as other topics.**\n",
    "   - Many examples from **topics 1, 2, 3, 5, and 10** are mistakenly categorized as **class 0**.\n",
    "   - A **better binary threshold** or **a filtering mechanism for low-confidence predictions** may help.\n",
    "\n",
    "2. **Class Imbalance is Well Handled**\n",
    "   - **Rare classes (e.g., 6, 7, 8, 9, 10) have good recall (~90%)**.\n",
    "   - Unlike SVM, which struggled with small classes, **BERT generalizes much better**.\n",
    "\n",
    "3. **Most Topics Have >90% Recall**\n",
    "   - Topics **1, 3, 4, 5, 6, 7, 8, 9, and 10** are very well classified, with recall approaching **97‚Äì99%** in some cases.\n",
    "   - Precision is slightly lower in topics like **5 and 6**, meaning there are some false positives.\n",
    "\n",
    "4. **Topic 2 is an Outlier**\n",
    "   - **Precision: 79%** (good)\n",
    "   - **Recall: 51%** (low)\n",
    "   - It suggests **many real topic 2 quotes were misclassified**. Some were misclassified as **topic 5 or topic 10**.\n",
    "   - Possible fix: **more training data for topic 2** or **better class weighting in loss function**.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. How This Compares to XGB+SVM**\n",
    "| Metric | **BERT-on-BERT** | **XGB+SVM** |\n",
    "|--------|----------------|-------------|\n",
    "| **Binary Accuracy** | **95%** | 86% |\n",
    "| **Multiclass Accuracy** | **76%** | 76% |\n",
    "| **Macro F1 (Multiclass)** | **0.78** | 0.62‚Äì0.64 |\n",
    "| **Weighted F1** | **0.72** | 0.64 |\n",
    "| **Recall (Multiclass)** | **0.83** | ~0.62 |\n",
    "| **Worst-Class Recall (Class 0 / False Positives from Binary)** | **16%** | ~10% |\n",
    "\n",
    "- **BERT-on-BERT maintains accuracy while massively improving F1.**\n",
    "- **Recall is much higher** in most categories, making **BERT generalizes much better than SVM.**\n",
    "- **Binary model improvements could help further**.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Next Steps to Improve BERT-on-BERT**\n",
    "1. **Reduce False Positives from the Binary Model**\n",
    "   - Class **0 recall is too low (16%)**, meaning some **out-of-topic quotes are misclassified in binary** and not corrected later.\n",
    "   - Try **adjusting the binary threshold** or **using confidence scores**.\n",
    "   - Another option: **train a secondary \"uncertainty filter\" to reduce noisy predictions**.\n",
    "\n",
    "2. **Balance Class Distribution**\n",
    "   - Class **2 (low recall)** might benefit from **more training data or class weighting**.\n",
    "   - If certain classes are **overrepresented in training**, the model may be biased.\n",
    "\n",
    "3. **Use a Better Loss Function**\n",
    "   - Try **Focal Loss** to **handle difficult classes (e.g., topic 2) more effectively**.\n",
    "   - This helps address class imbalance without manually oversampling.\n",
    "\n",
    "4. **Consider RoBERTa or DeBERTa for Even Higher Performance**\n",
    "   - If you want **even better accuracy**, **RoBERTa or DeBERTa** might help.\n",
    "   - They **outperform BERT in many classification tasks**.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Final Verdict**\n",
    "**‚úÖ BERT-on-BERT is the best approach you've tested.**  \n",
    "- It **outperforms XGB+SVM in every aspect**.\n",
    "- It has **better recall across most classes**, making it **more reliable for topic classification**.\n",
    "- Some minor issues remain, but **adjusting binary filtering & balancing training data** could push performance even higher.\n",
    "\n",
    "### **BERT-on-BERT WINS üéâ**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752b2933",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
